{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cycle Life Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stages and Operations in the Data Life Cycle\n",
    "\n",
    "<center>\n",
    "<img src= \"./assets/img/cycle_life_data.png\">\n",
    "<center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanig Data\n",
    "* Finding and handling missing values *NULL*, *NA* (How fill them)\n",
    "* Finding and handling *outliers*\n",
    "* Finding and handling *duplicated data* (How to determinate the duplicated data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Procesing\n",
    "* Transformation: Normalization, scaling y standarization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Datasets\n",
    "\n",
    "* Multimedia data:\n",
    "    \n",
    "    * Multimedia refers to data that represents audiovisual (video, images, audio) information. This data is usually encoded using one of the several standards for such media (for instance, JPEG for digital images or MPEG for audio/video)\n",
    "    *  Handling multimedia data requires specialized tools: in order to display the image or play the video or music, a special program (a ‘video/audio player’) that understands how the encoding works is needed\n",
    "\n",
    "* Alphanumeric:\n",
    "\n",
    "    * Collections of characters used to represent alphabetic (names, e.g. 'blue') and numeric individual e.g. 123.\n",
    "    * Most methods for data analysis have been developed to deal with alphanumeric data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Alphanumeric dataset** or simply dataset is a collection of data *items* or *rows* or *tuples* in database. *record* in Computer Science, *observation* in statistics, *entity* in *data points* in other contexs.\n",
    "> \n",
    "> Each item describes a real-world event, it consists of a group of related characteristics Such characteristics are called *attributes* in database, *variables* in statistics, *features* in Machine Learning and *properties* in other contex."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "|index|Age|Weight|\n",
    "|--|----|---|\n",
    "|1|15|19|\n",
    "|2|19|20|\n",
    "\n",
    "</center>\n",
    "\n",
    "Records or rows for items (r or s notation), and attributes (A notation)\n",
    "\n",
    "The number of records are *size* and the number of attributes are called *dimentionality*.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the exact *nature* of the records, datasets can be classified as *structured*, *semistructured*, and *unstructured*.\n",
    "\n",
    "we will focus mainly on structured (also called ‘tabular’) data, because this is the kind of data that relational databases handle best, but also because this is the kind of data that is most commonly assumed when talking about data analysis.\n",
    "\n",
    "Relational databases are also perfectly capable of handling semistructured and unstructured data,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes are make up by records, called the *schema of the records*.\n",
    "\n",
    "<center>\n",
    "\n",
    "|id|Age|address|\n",
    "|--|----|---|\n",
    "|1|15|42-street New york, 12205, New york|\n",
    "|2|19|4-street New Jersey, 10907, New Jersey|\n",
    "\n",
    "</center>\n",
    "\n",
    "The age attributes is a schema of records of simple values, while address is a schema of records of values complex."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Data\n",
    "\n",
    "* Structured data refers to datasets where all records share a common schema, i.e.\n",
    "they all have values for the same attributes. (Homogeneous).\n",
    "\n",
    "* Tabular data is structured data where all attributes are considered simple.\n",
    "\n",
    "* If a file is tabular, this is separated by commas or tabs. If this is separated by commas the file is called CSV (Comma Separated Values). This is *portable*, that is, this can be open in MAC or PC.  \n",
    "\n",
    "* The first row is header and it is separate by commmas, like that:\n",
    "    ```txt\n",
    "\n",
    "    first-name, last-name, age, gender\n",
    "    Mark, Sullivan, 80, M\n",
    "    Sue, Walk, 70, F\n",
    "    ```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "|flightid | year| month| day| dep_time| sched_dep_time| dep_delay| carrier| flight| origin |dest|\n",
    "|--|--|--|---|---|---|---|---|---|---|---|\n",
    "|1 |2013 |1 |1 |517 |515 |2 |\"UA\" |1545 |\"EWR\" |\"IAH\"|\n",
    "|2 |2013 |1| 1| 533| 529| 4| \"UA\"| 1714| \"LGA\"| \"IAH|\n",
    "\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semistructured Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semistructured data** is data where each record may have a different schema (also sometimes called heterogeneous data).\n",
    "\n",
    "In particular, some attributes may be optional, in that they are present in some records and not in others.\n",
    "\n",
    "Finally, some attributes may have as *value collections* of values as opposed to a *single one*. That is, the values of a attribute can be a list or array. \n",
    "\n",
    "In most real situations, semistructured data is used for datasets where attributes are not simple.\n",
    "\n",
    "Semistructured data includes data in XML and JSON.\n",
    "\n",
    "* **XML** describes the schema by using *tags*, \n",
    "    \n",
    "    * labels that are enclosed in *angular brackets* (< >). \n",
    "    * Tags always come in pairs, composed of an opening and a closing bracket \n",
    "    * They can be identified because the closing bracket is exactly like the opening one but with the addition of a backslash. \n",
    "    * The value of the attribute goes between the tag pair.`(<id>120080<\\id>)`.\n",
    "\n",
    "* **JSON** uses a different format for the same idea. \n",
    "\n",
    "    * Instead of tags, JSON uses *labels* for attributes, which are separated by a colon (:) from their value. \n",
    "    * Also, when an *attribute* denotes a collection, the values are enclosed in square brackets ([]), \n",
    "    * and when they denote a complex object, they are enclosed in curly brackets ({})\n",
    "    * This data type is most used for handly semi structure Data. MongoDB is a Data Base system NoSQL that can handly this structure of data.\n",
    "    \n",
    "      Azure Cosmos DB, ArangoDB, Amazon DynamoDB, Aerospike, Couchbase are other examples\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When complex attributes are present, the schema of a record can be described by what is called, in Computer Science, a **tree**: a *hierarchical structure* with a root or main part that is subdivided into parts. Each part can in turn be further subdivided."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A semistructured dataset can be considered, then, as a collection of tree-like\n",
    "structures.\n",
    "\n",
    "In most practical cases the objects in the collection will have a good deal in common, that is, they all will share at least part\n",
    "of the schema.\n",
    "\n",
    "This helps in dealing with semistructured data and makes it possible to put such data in a tabular format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classification of between structured and unstructure data is no rigit, since is possible accomadate irregular data into table.\n",
    "\n",
    "The complex data can be reduce into a value or give all records a schema with all attributes possible\n",
    "and leave empty cells for records that do not have values. Later is a a good idea if data is more homogeneous than heterogeneous.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Graph data**\n",
    "\n",
    "    * One particular, important case of semistructured data is **graph data**. Graph data (sometimes called network data) represents collections of objects that are connected (or linked, or related) to each other.\n",
    "\n",
    "    * A fundamental part of the dataset, then, is not just the objects themselves, but their connections. \n",
    "\n",
    "    *  In the context of graph data, the *objects* are called *nodes* or vertices, and the *links* are called *edges*.\n",
    "\n",
    "    * What makes graph data special is that the relationships between nodes are considered particularly important, so most analysis focuses on properties related to the edges, like finding whether two nodes N1 and N2 are connected (i.e. whether there is a sequence of edges, called a path, leading from N1 to another node N.\n",
    "\n",
    "    * Recall each node, that is, object is record that have other attributes apart from *link*, like heigh, or other.\n",
    "\n",
    "    * Sometimes the edges between nodes *have a direction*, that is, they represent a one-way relationship. \n",
    "\n",
    "    * Other times, the edges have no direction. Such relationship is undirected; a graph made up (exclusively) of undirected edges is called an *undirected graph*.\n",
    "\n",
    "    * Sometimes nodes in a graph represent different types of data. For instance, we\n",
    "    may have a graph where some nodes represent people and other nodes represent\n",
    "    books, and an edge between a person node and a book node represents the fact that\n",
    "    the person has read the book. In such cases, nodes usually have a type attribute,\n",
    "    and the graph is called a *typed graph*.\n",
    "\n",
    "    * It is interesting to note that, technically speaking, trees are special types of\n",
    "    graphs. In a tree, edges are directed in the direction parent → son, and each edge\n",
    "    must have only one parent (although it can have an arbitrary number of sons). Also,\n",
    "    trees do not admit cycles (paths that end up in the same node that they started)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unstructured data refers to data where the structure is not explicit, that is, no tags or markers to separate the records\n",
    "into attributes.\n",
    "\n",
    "* Most of the time, unstructured data refers to `text`, that is, to sentences written in some natural language.\n",
    "* Text data tends to come in one of the two ways:\n",
    "    * A collection of documents (usually called a corpus), the text in the documents themselves is the main target of analysis.(Here we use regular expression)\n",
    "    * We may have tabular data where one or more of the attributes are textual in nature. e.g. the attribute *description* of *sample_book.xml*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Systems\n",
    "A database is a collection of data stored on a computer. The data in a database is typically organized into records, such as employee records, medical records, sales records, etc. In actually, database is a set of tables.\n",
    "\n",
    "There is a STUDENT record (rown) for each student that has attended the university.\n",
    "Each record contains the student’s ID number, name, graduation year, and ID of\n",
    "the student’s major department"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Databases must be persistent. Otherwise, the records would disappear as soon as the computer is turned off.\n",
    "\n",
    "* Databases can be shared. Many databases, such as our university database, are intended to be shared by multiple concurrent users.\n",
    "\n",
    "* Databases must be kept accurate. If users cannot trust the contents of a database, it becomes useless and worthless.\n",
    "\n",
    "* Databases can be very large. The database of Fig. 1.1 contains only 29 records, which is ridiculously small. It is not unusual for a database to contain millions (or even billions) of records.\n",
    "\n",
    "* Databases must be usable. If users are not able to easily get at the data they want,\n",
    "their productivity will suffer, and they will clamor for a different product."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Storage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way to make a database persistent is to store its records in files. The type file used is text file.\n",
    "\n",
    "This approach is too inefficient to be useful. It has to rewrite on one hand, but on the other hand a long time to read."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-user Access\n",
    "\n",
    "When many users share a database, there is a good chance that they will be accessing some of its data files concurrently (simultaneously). This can generate some problems. One of them don't save correctly.\n",
    "A solution is limit the concurrence. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a database system must be able to detect when a user is about to perform an action that conflicts\n",
    "with an action of another user and then (and only then) block that user from executing until the first user has finished."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of domain\n",
    "In Python is known as data type. For given data record (row) a feature (or atribute) provides a value of its domain (set of values).\n",
    "\n",
    "* Nominal/Categorical Data/qualitative\n",
    "\n",
    "* Ordinal Data\n",
    "\n",
    "* Numerical Data/quantitative\n",
    "\n",
    "* When the data already exist\n",
    "  * We access it and export file in order to use other tool as Python.\n",
    "  * Or clean the data and preprocesing (EDA). and then useing Python.\n",
    "\n",
    "* The advantages of database over Python are:\n",
    "  * better manage of big data. It has more tools.\n",
    "  * databases offer strong control access: we can carefully monitor and limit access to the data (confidentiality).\n",
    "\n",
    "* If the data arrives periodically, or all at once but is later updated (that is, if data changes at all), the database offers tools to handle this evolution of data that do not exist in R or Python.\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Relational Data\n",
    "\n",
    "* Relational database systems store data in repositories called databases or schemas.\n",
    "\n",
    "* We use *database* for the data repository and reserve *schema* for the description of the structure\n",
    "\n",
    "* After starting a database server and connecting to it either create a database or connect it.\n",
    "\n",
    "* Supponse don't have a database, so we will create one.\n",
    "\n",
    "* Create the database\n",
    "\n",
    "  * The name of data base must make sense about what it contain\n",
    "  * Each server (user) can store severals database\n",
    "  * If this happen, we must specify which database to work\n",
    "\n",
    "* Create tables. Recall a database is a colection of tables.\n",
    "\n",
    "  * Components of a table are *name*, *schema*, *extension* and *primary key*.\n",
    "  * The name of *table* must unique inside a *database*. \n",
    "  * This is followed by a list of *attributes*. An *attribute* has a *name* (must unique among the attributes) and data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Create a database `AppUser`\n",
    "CREATE DATABASE AppUsers;\n",
    "\n",
    "-- Use a specific database `AppUser`\n",
    "USE AppUsers;\n",
    "\n",
    "-- Create table Employees \n",
    "CREATE TABLE Employees(\n",
    "EmplName CHAR(64),\n",
    "Age INT,\n",
    "BirthDate DATE,\n",
    "Salary FLOAT\n",
    "  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data type refers to the way in which a computer represents data values.\n",
    "\n",
    "* String: In CS sequence of characters made up of combination of number and letter (and white spaces if is allowed). Used to represent qualitative data.\n",
    "  * Examples: 'female', 'South America', '102'.\n",
    "  * Strings are also can be used to give names to database elements, like tables and\n",
    "attributes on them; these names are called *identifiers*. MySQL  allow identifiers with and without quotes; however, when not using quotes,\n",
    "certain restrictions apply: an identifier cannot use hyphens or start with a digit or\n",
    "an underscore or use a reserved SQL word (like CREATE, ATTRIBUTE, etc.).\n",
    "When using quotes, these restrictions are all lifted.\n",
    "  * *size*: The size can be fixed or no fixed.\n",
    "    ```mysql\n",
    "    name_col char varying(n) \n",
    "    ```\n",
    "    When a string value is shorter than the maximum size n, variable size strings adopt the size of the actual value.\n",
    "    ```mysql\n",
    "    name_col char(n) \n",
    "    ```\n",
    "    while fixed size\n",
    "strings are padded with whitespace (up to the maximum n characters). The max *n* is 256.\n",
    "  \n",
    "    In actually both return same results. And dennied more characters than n.\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Numbers: \n",
    "  * *int* can be \n",
    "    ```mysql\n",
    "    name_col int;\n",
    "    ```\n",
    "  * *double precision* data type offers 15 digit precision\n",
    "    ```mysql\n",
    "    name_col double precision;\n",
    "    ```\n",
    "  * *bigint* can store integer values between −263 and +263\n",
    "    ```mysql\n",
    "    name_col bigint; \n",
    "    ```\n",
    "  * *float*\n",
    "    ```mysql\n",
    "    name_col float; \n",
    "    ```\n",
    "  * *float8* offers 17 significant decimal digits\n",
    "    ```mysql\n",
    "    name_col float8; \n",
    "    ```\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Temporal information types:\n",
    "    \n",
    "  Usually, *date*, *time*, *timestamp*, and *interval* types\n",
    "  are supported. \n",
    "  * A date is a combination of year, month, and day of the month\n",
    "  that denotes a certain calendar day. The values are like '2020-05-10'.\n",
    "    ```mysql\n",
    "    name_col date\n",
    "    ```\n",
    "  * A time is composed of hour-minute-second\n",
    "  parts (some systems allow times with further precision, going down to tenths and\n",
    "  hundreds of second). The values are like '15:01:30'\n",
    "    ```mysql\n",
    "    name_col time\n",
    "    ```\n",
    "  * A timestamp is a combination of a date and a time, with all\n",
    "  the components of both. Finally, an interval is a pair of timestamps intended to\n",
    "  denote the beginning and end of a time period. The values are like '2020-05-10 15:10:30'\n",
    "    ```mysql\n",
    "    name_col timestamp\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Booleans: Values like True or False, but the output are 1 (True) or 0 (False)\n",
    "    ```mysql\n",
    "    name_col boolean\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE DATABASE IF NOT EXISTS FLIGHT;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "USE FLIGHT;\n",
    "\n",
    "CREATE TABLE NY_FLIGHTS(\n",
    "    flightid int,\n",
    "    col_year int,\n",
    "    col_month int,\n",
    "    col_day int,\n",
    "    dep_time int,\n",
    "    sched_dep_time int,\n",
    "    dep_delay int,\n",
    "    arr_time int,\n",
    "    sched_arr_time int,\n",
    "    arr_delay int,\n",
    "    carrier char(2),\n",
    "    flight char(4),\n",
    "    tailnum char(6),\n",
    "    origin char(3),\n",
    "    dest char(3),\n",
    "    air_time int,\n",
    "    distance int,\n",
    "    col_hour int, \n",
    "    col_minute int, \n",
    "    time_hour timestamp);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Information about the date of the flight is expressed by 3 attributes, year, month,\n",
    "and day.\n",
    "* It is easy to extract parts from the original value and to concatenate (put\n",
    "together) strings.\n",
    "* If breaking the value into parts is not an easy task (for instance, in real life dealing with dates is usually much more complicated),\n",
    "* Many times, the granularity level at which to express the data is decided for us"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A row represents a record, object, or event; it gives a sequence of data values, one value\n",
    "for each attribute in the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- INSERT ROW A DEFAULT ORDER\n",
    "INSERT INTO ny_flights VALUES(1, 2013, 1, 1, 517, 515, 2, 830, \n",
    "819, 11, \"UA\", 1545, \"N14228\", \"EWR\", \"IAH\", 227, 1400, 5, 15,\n",
    "'2013-01-01 05:00:00');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that, since each value must match some attribute in the schema, we must\n",
    "pair values with attributes. This is done in one of the two ways:\n",
    "    * Simply enumerating the values will pair them up with attributes following the\n",
    "order used in the CREATE TABLE statement (the default order). \n",
    "    * Specifying a customer order in the INSERT statement.\n",
    "    * Given a table **T**, and A, B and C the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- INSERT A SPECIFIC ORDER\n",
    "INSERT INTO ny_flights(col_day ,col_year) VALUES\n",
    "(3, 2010);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The case where data does not exactly follow the schema. SQL offers some flexibility\n",
    "for the cases where we have incomplete data (some values are missing): a special\n",
    "marker, called the NULL marker, can be used to signal that a value is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- INSERT ROW A DEFAULT ORDER BY INCOMPLETE\n",
    "INSERT INTO ny_flights VALUES(2, 2013, 1, 1, null, null, 4,\n",
    "850, 830, 20, \"UA\", 1714, null, \"LGA\", \"IAH\", 227, null, 5,\n",
    "29, '2013-01-01 05:00:00');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To destroy a table \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- tablename that we can drop or eliminate\n",
    "DROP TABLE tablename;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All systems provide a method to retrieve information about a table, including its schema. In MySQL, there is a command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TO SHOW INFORMATION OF SOME ATTRIBUTES\n",
    "SHOW TABLES FROM FLIGHT;\n",
    "\n",
    "-- show columns that belong to NY NY_FLIGHTS table\n",
    "USE FLIGHT;\n",
    "SHOW COLUMNS FROM NY_FLIGHTS;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though database tables can be depicted as described in the previous chapter,\n",
    "it is important to note that the order of the columns or the rows is completely\n",
    "irrelevant. In other words, a table with the same number, type, and names of\n",
    "columns in a different order would be considered to have the same schema. And\n",
    "two tables over the same schema with the same rows, even in different order,\n",
    "would be considered to have the same data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Identifier of a row. When we create tables, we must identify and declare keys for each table.\n",
    "* This must be unique (what should never happen is that a key is present in two (or more) different rows.) and it **one** of the columns can be a mix from a combination of attributes.\n",
    "* We may have situations in which a table has more than one key. In such a scenario, we choose one key to be the **primary key** of the table.\n",
    "* Sometimes we need to create the key called idenfier (*id*). Usually, an id is simply an integer that is assigned to each row by giving a 1 to the first row inserted in the table, a 2 to the second row, etc\n",
    "* They technically satisfy all the requirements\n",
    "of a key, but (unlike other attributes) do not refer to anything in the real world.\n",
    "* Therefore, they are useless for most data analyses. Second, if there is a key in a\n",
    "table, and we decide to create an id, the key is still there, and if it goes undetected\n",
    "(so it is not even declared as UNIQUE) this may lead to some problems later on.\n",
    "> The moral of the story is that we should not rush to create ids; rather, we should\n",
    "examine the existing attributes carefully to identify any possible keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TABLE CREATED WITHOUT PRYMARY KEY\n",
    "CREATE TABLE books (\n",
    "book_id INT,\n",
    "title VARCHAR(100) NOT NULL,\n",
    "author VARCHAR(100),\n",
    "publisher VARCHAR(100),\n",
    "num_pages INT);\n",
    "\n",
    "-- WE CAN MODIFY A TABLE AND USING AN ATTRIBUTE LIKE KEY\n",
    "ALTER TABLE books ADD PRIMARY KEY AUTO_INCREMENT (book_id);\n",
    "\n",
    "-- OR WE CAN CREATE KEY WHEN WE CREATE THE TABLE\n",
    "DROP TABLE books; -- FIRST DROP THE TABLE CREATED ALREADY\n",
    "-- NOW WE CREATED AGAIN\n",
    "CREATE TABLE books (\n",
    "book_id INT NOT NULL AUTO_INCREMENT PRIMARY KEY, -- <\n",
    "title VARCHAR(100) NOT NULL,\n",
    "author VARCHAR(100),\n",
    "publisher VARCHAR(100),\n",
    "num_pages INT);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setting tell us:\n",
    "* If a (primary) key contains nulls, the system\n",
    "cannot ascertain whether it is a correct key or not. \n",
    "* If we declare a (primary) key and\n",
    "misuse it (by entering the same value of the so-called key in two distinct tuples), the\n",
    "system will reject the insertion because it violates the condition of being a key."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Data into Tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Wide (Stacked) and*\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "|Person| Age| Weight|\n",
    "|-|-|-|\n",
    "|Bob| 32| 128|\n",
    "|Alice| 24| 86|\n",
    "|Steve| 64| 95|\n",
    "\n",
    "</center>\n",
    "\n",
    "*Narrow (Unstacked) Data*\n",
    "\n",
    "<center>\n",
    "\n",
    "|Person| Variable| Value|\n",
    "|-|-|-|\n",
    "|Bob| Age| 32|\n",
    "|Bob| Weight| 128|\n",
    "|Alice| Age| 24|\n",
    "|Alice| Weight| 86|\n",
    "|Steve| Age| 64|\n",
    "|Steve| Weight| 95|\n",
    "\n",
    "</center>\n",
    "\n",
    "In the wide table the key is Person, suggesting that each row describes a person, while in the narrow table the key is (Person, Variable), a strange combination that is a hint that something is not quite right.\n",
    "\n",
    "*Narrow/stacked Data*\n",
    "\n",
    "<center>\n",
    "\n",
    "|Product| S| M| L|\n",
    "|-|-|-|-|\n",
    "|Shirt| 12.4| 23.1| 33.3|\n",
    "|Pants| 3.3| 5.3| 11.0|\n",
    "\n",
    "</center>\n",
    "\n",
    "*Wide/unstacked Data*\n",
    "\n",
    "<center>\n",
    "\n",
    "|Product| Size| Price|\n",
    "|-|-|-|\n",
    "|Shirt| S| 12.4|\n",
    "|Shirt| M| 23.1|\n",
    "|Shirt| L| 33.3|\n",
    "|Pants| S| 3.3|\n",
    "|Pants| M| 5.3|\n",
    "|Pants| L| 11.0|\n",
    "\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proper table data (called *tidy data*) is **wide/unstacked**. To achieve this, it follows certain conventions\n",
    "* All values are in the data cells, never in the schema (as a name of a features).\n",
    "  * For instance, S (small), M (medium), and L (large) are values of attribute size, so they should be in data cells, not part of the schema.\n",
    "* All attributes are in the schema, never in the data\n",
    "  * For instance, Age and Weight are attributes of a person, so they should be in the schema, not in the data\n",
    "* Each row corresponds to a data record (observation, in statistics) and each column\n",
    "to an attribute (variable, in statistics). \n",
    "  * Intuitively, in a table about people we want each row to describe a person\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Schemas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Heterogeneous Data:\n",
    "\n",
    "  We may have records in our dataset that are not completely homogeneous: while sharing many common attributes, some entities may be somewhat different. This may be due to some attributes being present only under certain circumstances and not applying to each record. These values are filled with NULLs or split the data, each one must be full filled. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the table\n",
    "<center>\n",
    "<img src= \"./assets/img/Captura_db_schema_01.PNG\">\n",
    "</center>\n",
    "\n",
    "To\n",
    "\n",
    "<center>\n",
    "<img src= \"./assets/img/Captura_db_schema_02.PNG\">\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-valued Attributes\n",
    "\n",
    "we have that some records are characterized by attributes that have several values for a given record (these are called multi-valued attributes in database textbooks).\n",
    "\n",
    "* In business applications, it is common that we think in terms of \"transactions,\" each one of each involves a set of products.\n",
    "* To describe this data, we usually assign an identifier to each transaction and associate all involved products with the transaction they belong to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src= \"./assets/img/Captura_db_schema_03.PNG\">\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, even though `Transaction-id` is meant to identify the transaction, the key of this table is `(Transaction-id, Product)`, since the key needs to be unique for each row."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**\n",
    "\n",
    "Consists of spreading the data into several tables by separating the multi-valued attributes to the single-valued ones.\n",
    "* Here the multi-valued attribute are Date, Time and Store."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "<img src= \"./assets/img/Captura_db_schema_04.PNG\">\n",
    "</center>\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src= \"./assets/img/Captura_db_schema_05.PNG\">\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The key of the first table is Transaction-id, as it stores each transaction information in one row. This is possible because only single-valued attributes are used in the schema of the table. Conversely, the second table stores the multi-valued attribute Product and (as stated above) has a different key. \n",
    "* However, it keeps a copy of Transaction-id so it can connect records to the transactions as described in the\n",
    "previous table. \n",
    "* Copies of primary keys are used in databases to keep connections in data and are called foreign keys."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The normalization avoid *anomalies*, situations where the database may be left in an inconsistent state by changes (inconsistency here means that data in some tuples contradicts the data in other tuples)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The strategy of normalization is also used in this third scenario: situation in which we must deal with complex data.\n",
    "\n",
    "The most scenarios the connections among event/entities are binary (they involve two events/entities) and, for the purposes of database design, they can be classified in one of the three types:\n",
    "\n",
    "* A collection of record I1 has a **one-to-one** relationship with a collection of records I2 if a record in I1 is related to only one record in I2 and vice versa (a record in I2 is related to only one record in I1). \n",
    "  * For instance, assume we have a demographic database with data about people, but also data about addresses (the exact longitude/latitude, state, etc.) and that each person is associated with one address and each address with one person.\n",
    "* A collection of record I1 has a **one-to-many** relationship with a collection of records I2 if a record in I1 is related to only one record in I2 but a record in I2 may be related to several records in I1.\n",
    "  * For instance, we can assume that some customers have placed several orders.\n",
    "\n",
    "* A collection of record I1 has a **many-to-many** relationship with a collection of records I2 if a record in I1 may be related to several records in I2 and vice versa (a record in I2 may be related to several records in I1).\n",
    "  * For instance, assume we have a dataset of different chemical compound suppliers and another one of chemical laboratories. Each laboratory buys from several suppliers; each supplier sells to several laboratories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we want each event/entity (for instance transaccion/customer) to have its own table (table for transaccions and table for customer).Here we have other example.\n",
    "\n",
    "* A table for *Patients* may have *attributes* like *Name*, *date-of-birth*, *insurance-company*, etc., while a table\n",
    "for *Studies* may have *attributes* like *date-started*, *sponsor*, and so on.\n",
    "\n",
    "In databases the relationship between these tables is achieved when the *primary key* table of entity is copied to table event/entity (primary key copied to another table is called a foreign key in SQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--------------------------------------------------------\n",
    "-------------------- ONE-TO-ONE -----------------------\n",
    "--------------------------------------------------------\n",
    "\n",
    "-- Create a table  PEOPLE \n",
    "\n",
    "CREATE TABLE PEOPLE (\n",
    "    PRIMARY KEY(Name_people),\n",
    "    Name_people VARCHAR(64),\n",
    "    first_name VARCHAR(32),\n",
    "    last_name VARCHAR(32)\n",
    "    );\n",
    "\n",
    "-- Create a table named ADDRESS\n",
    "-- with primary key (street_number, street_name, city).\n",
    "-- That mean we can't insert two address for a person\n",
    "\n",
    "CREATE TABLE ADDRESS (\n",
    "    street_number INT,\n",
    "    street_name VARCHAR(128),\n",
    "    city VARCHAR(64),\n",
    "    Name_people VARCHAR(64),\n",
    "    FOREIGN KEY(Name_people) REFERENCES PEOPLE(Name_people), \n",
    "    PRIMARY KEY(street_number, street_name, city)\n",
    "    );\n",
    "\n",
    "-- WHEN WE INSERT THE VALUES, THESE MUST HAVE \n",
    "-- THE SAME ORDER IN WHICH WERE CREATED \n",
    "\n",
    "INSERT INTO PEOPLE VALUES \n",
    "('Dante Toribio', 'Dante', 'Toribio'),\n",
    "('Sofía Mendez', 'Sofía', 'Mendez'),\n",
    "('Valeria Rojas', 'Valeria', 'Rojas'),\n",
    "('Gregorio Zull', 'Gregorio', 'ZUll');\n",
    "\n",
    "INSERT INTO ADDRESS VALUES\n",
    "(1001,'UNION', 'LIMA' ,'Dante Toribio'),\n",
    "(1005, 'UNION','LIMA' ,'Sofía Mendez'),\n",
    "(5004, 'SAN MARCOS', 'CHICLAYO' ,'Valeria Rojas'),\n",
    "(2001,'PATRON' , 'HUACHO' ,'Gregorio Zull');\n",
    "\n",
    "-- -----------------------------------------------------\n",
    "-- ---------------- ONE-TO-MANY-------------------------\n",
    "-- -----------------------------------------------------\n",
    "\n",
    "CREATE TABLE CUSTOMER(\n",
    "\n",
    "    PRIMARY KEY(CustomerID),\n",
    "    CustomerID int,\n",
    "    NameCustomer char(64),\n",
    "    YearCustomer int\n",
    ");\n",
    "\n",
    "create table ORDERS(\n",
    "    primary key(OrderID),\n",
    "    foreign key(CustomerID) references CUSTOMER(CustomerID) on update cascade,\n",
    "    OrderID INT,\n",
    "    CustomerID int, \n",
    "    DateCustomer char(32)\n",
    ");\n",
    "\n",
    "-- In this order are created and filled (but in a next step).\n",
    "\n",
    "INSERT INTO CUSTOMER VALUES\n",
    "(1012, 'Fabiola', 20),\n",
    "(1013, 'Keyla', 30),\n",
    "(1014, 'Lucía', 50);\n",
    "\n",
    "\n",
    "INSERT INTO ORDERS VALUES\n",
    "(123, 1012, '15/10/2021'),\n",
    "(124, 1012, '15/10/2021'),\n",
    "(125, 1013, '15/10/2021'),\n",
    "(126, 1013, '15/10/2021'),\n",
    "(127, 1014, '15/10/2021');  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom is explained the command `on update cascade`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **the type and number of attributes in the foreign key must match the primary key it copies**:\n",
    "\n",
    "* If the primary key is a single attribute of type integer (as in this example), the foreign key must be a single attribute of type integer. \n",
    "\n",
    "* If the primary key were made of two attributes, one of type string and one of type date, any foreign key should also have two attributes, one of type string and one of type date. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **order in which is filled the tables** is firstly the table that contains only *primary key* (table entity). Then, the table that contains both *primary key* and *foraign key* (table event/entity). \n",
    "\n",
    "To help keep consistency, a database offers several (optional) commands when declaring foreign key.\n",
    "  * ON [DELETE|UPDATE] [CASCADE|SET NULL|SET DEFAULT|RESTRICT]\n",
    "\n",
    "\n",
    "`on update cascade` means that if we do some changes in the primary key for the table entity, these changes is spreaded in other tables that contains this primary key but as foraing key.\n",
    "\n",
    "we can imagine what the command `on delete cascade` do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- The command to updated values in a table.\n",
    "UPDATE CUSTOMER SET CustomerID=1015 WHERE CustomerID =1012 ;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransactionID is the primary key of the table describing transactions, but is also part of the schema (and part of the key) of the table that relates transactions with their records. Hence, in the second table above TransactionID is a foreign key.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of Many-to-Many Relationships**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE SUPPLIER (\n",
    "    PRIMARY KEY(IdSupName),\n",
    "    IdSupName INT,\n",
    "    SupName  character(100));\n",
    "\n",
    "CREATE TABLE LABORATORY ( \n",
    "    PRIMARY KEY(IdLabName),\n",
    "    IdLabName INT, \n",
    "    LabName character(100));\n",
    "\n",
    "CREATE TABLE COMPOUND ( \n",
    "    PRIMARY KEY(IdCompName),\n",
    "    IdCompName INT, \n",
    "    CompName character(100));\n",
    "\n",
    "CREATE TABLE BUYS (\n",
    "    PRIMARY KEY (IdSupName, IdLabName, IdCompName),\n",
    "    FOREIGN KEY(IdLabName) REFERENCES LABORATORY(IdLabName) on update cascade,\n",
    "    FOREIGN KEY(IdSupName) REFERENCES SUPPLIER(IdSupName) on update cascade,\n",
    "    FOREIGN KEY(IdCompName) REFERENCES COMPOUND(IdCompName) on update cascade,\n",
    "    IdSupName INT,\n",
    "    IdLabName INT,\n",
    "    IdCompName INT,\n",
    "    Amount INT,\n",
    "    BuyDate char(32));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the primary key:\n",
    "\n",
    "`The same seller can't sell the same compound twice at the same laboratory, but the seller can sell the same product to different laboratory and the a laboratory can buy the same compound to different sellers `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO SUPPLIER VALUES \n",
    "(1010, 'Silva Medic S.A.C.'),\n",
    "(1011, 'Feros Medic S.R.L.');\n",
    "\n",
    "INSERT INTO LABORATORY VALUES\n",
    "(2020, 'Suiza Lab S.A.'),\n",
    "(2021, 'New Lab S.A.');\n",
    "\n",
    "INSERT INTO COMPOUND VALUES\n",
    "(3030, 'PANADOL 300ML'),\n",
    "(3031, 'PARACETAMOL 500ML');\n",
    "(3032, 'ALTANGINA 800ML');\n",
    "\n",
    "INSERT INTO BUYS VALUES\n",
    "(1010, 2020, 3030, 20, '28/01/2021'),\n",
    "(1010, 2020, 3031, 80,'28/01/2021'),\n",
    "(1010, 2021, 3030, 5,'28/01/2021'),\n",
    "(1010, 2021, 3031, 10,'28/01/2021'),\n",
    "(1011, 2020, 3032, 40,'28/01/2021'),\n",
    "(1011, 2021, 3032, 60,'28/01/2021');\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a many-to-many relationship we create a separate table where we copy, as foreign keys, the primary keys of the tables representing the records involved in the relationships. These two foreign keys, together, are the primary key of this new table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with the time.\n",
    "\n",
    "Temporal information can make the schema of a database more complex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE PARTICIPANT (\n",
    "    PId INT PRIMARY KEY);\n",
    "\n",
    "CREATE TABLE MEASUREMENTS (\n",
    "    FOREIGN KEY(PId) REFERENCES PARTICIPANT(PId) ON UPDATE CASCADE,\n",
    "    PId INT,\n",
    "    Date_test DATE,\n",
    "    Time_test TIME,\n",
    "    blood_pressure_sys INT,\n",
    "    blood_pressure_dia INT,\n",
    "    respiratory_rate INT,\n",
    "    heart_rate INT,\n",
    "    temperature FLOAT,\n",
    "    PRIMARY KEY (PId, Date_test, Time_test));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Types of Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we discuss how to store semistructurre and unstructured data in the database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XML and JSON Data**\n",
    "* XML and JSON are used with hierarchical or tree structured data.\n",
    "* Hierarchical data can be stored inside the database in one of the two ways: by flattening it or by using the new XML or JSON data types added to the SQL standard.\n",
    "  * In this first case, we can put hierarchies in tables in one of the two ways:\n",
    "normalized and unnormalized. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data In and Out of the Database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a table or tables have been created in a database, it is time to bring in the data\n",
    "\n",
    "### Importing and Loading Data\n",
    "* The two most common ways to put data in the tables of a database are insert tuples or to load in bulk.\n",
    "\n",
    "* If data is already in a file, it is possible to load the data into the database in one swoop.\n",
    "\n",
    "Generally, one specifies the location of the file in the computer, the table to load into, and provides a description of how the lines in the file are to be broken down into the values that make up a row/tuple by indicating how values are separated from each other and a few other characteristics.\n",
    "\n",
    "The loader will read the file line by line and split each line according to the given instructions. It will expect that there are as many values on each line as there are on the schema of the table and of the right type.\n",
    "\n",
    "However, datasets often come with dirty data and this creates a problem when trying to load the data in the database. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing values**\n",
    "* Lack of values can be manifested in a file in two ways: by an *empty field* (that is, two consecutive appearances of the delimiter) or by some *marker* (markers like ‘\\N’ or ‘NA,’ for ‘Not Available,’ are particularly common). \n",
    "* The solution depend on system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The string** can included commas, dots, sigle quotes or double quotes. `\"Spring, Summer, Fall, Winter ... and Spring\"` for example.\n",
    "\n",
    "**Dates and times** are typically recognized by most database systems if they follow\n",
    "a certain format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An IBD file is a MySQL table created by the InnoDB database engine. It contains a table-specific tablespace and index data. IBD files are created when MySQL's innodb_file_per_table option is enabled, which it is by default.\n",
    "> \n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load file csv file using an API provided by pandas, SQLAlchemy, ... (in actually any file that python can read)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sql file\n",
    "\n",
    "mysql -u [username] -p -h [port] [databasename] < [data.sql]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export sql file\n",
    "\n",
    "mysqldump -u [username] -p [databasename] > [filename.sql]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Delete Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data is in the table, we may need to modify some of it. Modifications are accomplished with two SQL commands: `DELETE` and `UPDATE`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- UPDATE THE TABLE tableName \n",
    "UPDATE TABLE tableName \n",
    "SET colName = something\n",
    "WHERE condition;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- DELETE THE OBSERVATIONS FROM THE TABLE THAT MATCH THE CONDITION.\n",
    "DELETE FROM tableName \n",
    "WHERE condition;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DELETE statement works as follows: on each row of the table, the condition is checked. If it returns True, then the row is deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "USE COMPLEX;\n",
    "\n",
    "DELETE FROM BUYS\n",
    "WHERE IdCompName = 3032;\n",
    "\n",
    "DELETE FROM COMPOUND\n",
    "WHERE IdCompName = 3032;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposing that the name of the compound is not allow to sell-buy to the market, so this must removed. Since the IdCompName is a key foraign in the the table BUYS, first we must delete the IdCompName in the table BUY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TO UPDATE VALUES\n",
    "UPDATE COMPOUND \n",
    "SET IdCompName = 3036\n",
    "WHERE IdCompName = 3031;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command update the IdCompName from 3031 to 3036, not only in the table COMPOUND but in BUYS as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UPDATE command is used to change existing data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Mysql workbench we will create (import) a table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First we create a database\n",
    "    ``` sql\n",
    "    CREATE DATABASE data_product;\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a table with come commant SELECT\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://dev.mysql.com/doc/refman/8.0/en/create-table-select.html\n",
    "\n",
    "* https://dev.mysql.com/doc/refman/5.7/en/create-table-like.html\n",
    "\n",
    "* https://dev.mysql.com/doc/refman/8.0/en/create-temporary-table.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basic SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- The SELECT statement\n",
    "-- SYNTAX\n",
    "\n",
    "SELECT result_list\n",
    "FROM data_sources\n",
    "WHERE condition;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* result_list: This list determines what gets retrieved from the database like `title`, `code_num`\n",
    "* data_sources: denotes where the data examined for this query comes from. This can be other query called inner query as well.\n",
    "* Where: is the condition.`and`, `or`, `not`.\n",
    "  * For number, arithmetic conditions (comparisons using\n",
    "<, ≤, =, >, ≥) are common\n",
    "  * For string can be used regular expression when we work with string. But we can use \n",
    "  `like` `%dan%` the meaning is contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE DATABASE IF NOT EXISTS LINIO_SAMPLE;\n",
    "-- IMPORT DATA BY WIZARD FROM \n",
    "-- C:\\Users\\LENOVO\\Desktop\\python_course\\PySpark\\Learning\\assets\\files\n",
    "-- PRODUCTS\n",
    "-- SALES_NORMALIZE\n",
    "-- USERS\n",
    "\n",
    "USE LINIO_SAMPLE; "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to rename a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "RENAME tableName  TABLE TO newTableName;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT PRODUCT_ID, (OLD_PRICES - CURRENT_PRICES)/OLD_PRICES AS DISCOUNT\n",
    "FROM PRODUCTS\n",
    "WHERE CURRENT_PRICES > 50;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to sort the result. Note we need to use `GROUP BY` on `PRODUCT_ID`(which is unique), then we use `ORDER BY`. This technique is for any table whose need to be order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT PRODUCT_ID, (OLD_PRICES - CURRENT_PRICES)/OLD_PRICES AS DISCOUNT\n",
    "FROM PRODUCTS\n",
    "WHERE CURRENT_PRICES > 50\n",
    "GROUP BY PRODUCT_ID\n",
    "ORDER BY DISCOUNT;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM (SELECT * FROM USERS\n",
    "WHERE AGES>40) AS AGES_40\n",
    "WHERE DEPARTMENT='Lima' ;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe Lima is no written always with Capitalize, in this case, we use `RegEx` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM (SELECT * FROM USERS\n",
    "WHERE AGES>40) AS AGES_40\n",
    "WHERE DEPARTMENT REGEXP '^[Ll]ima$' ;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another available predicate (both for numbers and strings) is `IN`. It compares an\n",
    "attribute to a list of constants; if the value of the attribute equals any of the constants,\n",
    "the `IN` predicate is satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(*)\n",
    "FROM  USERS\n",
    "WHERE DEPARTMENT IN ('Ica', 'Puno');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- BETWEEN\n",
    "SELECT COUNT(*)\n",
    "FROM PRODUCTS\n",
    "WHERE CURRENT_PRICES BETWEEN 35 AND 50;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sometimes we may get duplicates, that is, two or more rows that contain the exact same values.\n",
    "\n",
    "SQL provides the keyword `DISTINCT` that can be added to SELECT to signal to the system that we want duplicates eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- RETURN THE TABLE THAT CONTAIN UNIqUE PRODUCT_ID\n",
    "SELECT DISTINCT(PRODUCT_ID) AS ONLYS\n",
    "FROM SALES_NORMALIZE;\n",
    "\n",
    "-- COUNT UNIqUE PRODUCT_ID\n",
    "WITH ONLYS_TABLE AS (SELECT DISTINCT(PRODUCT_ID) AS ONLYS\n",
    "\t\t\t\t\t  FROM SALES_NORMALIZE)\n",
    "SELECT COUNT(ONLYS) AS UNIQUE_ID_PRODUCTS\n",
    "FROM ONLYS_TABLE;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cartesian Product**\n",
    "\n",
    "<center>\n",
    "<img src= \"./assets/img/cartesian_01.PNG\">\n",
    "\n",
    "<img src= \"./assets/img/cartesian_02.PNG\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because *CPs* are associative, we can do this in any order, and the result is still the same"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way of add a collumn in a table is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "WITH T AS (SELECT MAX(CURRENT_PRICES) AS CURRENT_PRICES_MAX\n",
    "\t\t   FROM PRODUCTS)\n",
    "SELECT CURRENT_PRICES/CURRENT_PRICES_MAX AS RATIO_CURRENT_PRICES\n",
    "FROM PRODUCTS, T;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we add the max value fo price in the table `PRODUCTS`\n",
    "\n",
    "CPs are only needed in certain occasions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "USE WORLD;\n",
    "\n",
    "SELECT *\n",
    "FROM COUNTRY JOIN CITY ON (COUNTRY.CODE= CITY.COUNTRYCODE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- -----------------------------------------------\n",
    "-- JOIN MORE ELEGANT = CARTESIAN PRODUCT + FIRTER\n",
    "-- -----------------------------------------------\n",
    "SELECT * \n",
    "FROM country  AS CountryTab , city as CityTab\n",
    "WHERE (CountryTab.Code = CityTab.CountryCode);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(Standard) functions`. These are functions that are applied to the value of some attribute in a row and yield a (single) result. Since most attributes are of type string, numerical, or date, the typical system has string functions (that is, functions that take a string value, and perhaps some additional parameters, and manipulate the value in certain ways), numerical functions (including typical arithmetic functions), and date functions (that is, functions that take a date value, and perhaps some additional parameters, and manipulate the date in certain ways).\n",
    "\n",
    "* `POW(m,n)`: $m^n$\n",
    "* `SQRT(m)`: $\\sqrt{m}$\n",
    "* `POW(m,1/n)`: $\\sqrt[n]{m}$\n",
    "* `LOG(m, b)`: $log_{b}m$\n",
    "* `MOD(m,n)`, which returns the remainder of dividing m by n (and is often used\n",
    "to compute modular arithmetic)\n",
    "* `CEIL(m)`, which returns the ceiling of m (that is, the least integer that is greater\n",
    "than or equal to m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate functions. These are functions that take in a collection of values and produce a single result. Aggregate functions can only be used in the SELECT\n",
    "clause.\n",
    "* `AVG` (average or mean), \n",
    "* `SUM` (sum of values), \n",
    "* `COUNT` (count number of values), \n",
    "* `MIN` (minimum), and \n",
    "* `MAX` (maximum)\n",
    "* `std` or `stddev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- STRING FUNCTIONS\n",
    "\n",
    "USE WORLD;\n",
    "SHOW TABLES from WORLD;\n",
    "\n",
    "-- LOWER\n",
    "SELECT LOWER(COUNTRYCODE)\n",
    "FROM CITY;\n",
    "\n",
    "-- CONCAT\n",
    "SELECT  CONCAT(CITY.NAME , '-', CITY.DISTRICT) AS NAME_DISTRICT\n",
    "FROM CITY;\n",
    "\n",
    "-- TRIM => TO  REMOVE THE WHITE SPACES AT LEFT AND RIGHT\n",
    "-- LTRIM => TO  REMOVE THE WHITE SPACES AT LEFT\n",
    "-- RTRIM => TO  REMOVE THE WHITE SPACES AT RIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- --------------\n",
    "-- CONDITIONALS\n",
    "-- --------------\n",
    "-- THE DEMOGRAPHY OF A COUNTRY RESPECT TO CONTINENT (IN %)\n",
    "-- COUNT THE COUNTRIES THAT OVER 1%\n",
    "WITH TY AS\t(WITH T AS (SELECT CONTINENT AS CONTINENT_T, SUM(POPULATION) AS POPULATION_BY_CONTINENT\n",
    "\t\t\t            FROM COUNTRY\n",
    "\t\t\t            GROUP BY CONTINENT)\n",
    "\t\t\tSELECT CONTINENT, COUNTRY.NAME, (\n",
    "\t\t\t\tCASE WHEN POPULATION/POPULATION_BY_CONTINENT>0.01 THEN 1 ELSE 0 END) AS POPULATION_GT_1_BY_CONTINENT\n",
    "\t\t\tFROM COUNTRY JOIN T ON COUNTRY.CONTINENT = T.CONTINENT_T)\n",
    "SELECT CONTINENT, SUM(POPULATION_GT_1_BY_CONTINENT) AS COUNT_1\n",
    "FROM TY\n",
    "GROUP BY CONTINENT\n",
    "ORDER BY COUNT_1 DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GROUP BY` clause tells the system to partition the table into groups: then, any aggregates or actions in the SELECT clause are carried out within each group. \n",
    "\n",
    "The table returned as final result contains one row per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT REGION, COUNT(*) as HIST\n",
    "FROM COUNTRY\n",
    "WHERE CONTINENT= 'Asia'\n",
    "GROUP BY REGION; "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any SQL query with a `GROUP BY`, the SELECT can only mention attributes that appear in the GROUP BY or `aggregations`of some columns. \n",
    "\n",
    "This is because the result of a query is a table consisting of one row per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- This recipe is invalidate - it will return error\n",
    "-- THIS RECIPE RETURN ERROR, SINCE POPULATION DON'T APPEAR IN LIKE A GRUPING COLUMN NEITHER\n",
    "-- IS NOT AN AGGREGATION\n",
    "SELECT REGION, POPULATION, COUNT(*) as HIST\n",
    "FROM COUNTRY\n",
    "WHERE CONTINENT= 'Asia'\n",
    "GROUP BY REGION; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Distribution of countries by Asia regions\n",
    "\n",
    "WITH T AS (SELECT COUNT(*) AS COUNT_REGION_ASIA\n",
    "\t\t\tFROM COUNTRY\n",
    "            WHERE CONTINENT = 'Asia')\n",
    "SELECT REGION, COUNT(*)/COUNT_REGION_ASIA AS DIST\n",
    "FROM COUNTRY, T\n",
    "WHERE CONTINENT = 'Asia'\n",
    "GROUP BY REGION;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- GROUP BY and CASE\n",
    "\n",
    "WITH T AS (select Continent, LifeExpectancy, (case when LifeExpectancy > 80 then \"High\" \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\twhen LifeExpectancy > 70 then \"Medium\"\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\twhen LifeExpectancy > 50 then \"Low\" \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\telse \"Bad\" end) as Classification\n",
    "\t\t\tfrom country)\n",
    "SELECT Continent, Classification, COUNT(Classification)\n",
    "FROM T\n",
    "GROUP BY Continent, Classification\n",
    "ORDER BY Continent, Classification;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT CONTINENT, SUM(CASE WHEN LifeExpectancy < 50 THEN 1 ELSE 0 end ) as  \"Bad\",\n",
    "\t\t\t\t  SUM(CASE WHEN LifeExpectancy BETWEEN 50 and 70 THEN 1 ELSE 0 end) as  \"Low\",\n",
    "\t\t\t\t  SUM(CASE WHEN LifeExpectancy BETWEEN 70 and 80 THEN 1 ELSE 0 end) as  \"Medium\",\n",
    "\t\t\t\t  SUM(CASE WHEN LifeExpectancy > 80 THEN 1 ELSE 0 end ) as  \"High\"\n",
    "FROM COUNTRY\n",
    "GROUP BY CONTINENT;\n",
    "\n",
    "-- STEPS OF THE COMPUTATIONS\n",
    "-- 1. GROUP BY CONTINENT\n",
    "-- 2. BY EACH GROUP COMPUTE THE CASE WHEN\n",
    "-- 3. COMPUTE THE AGGREGATION FUNCTION SUM\n",
    "-- 4. THIS IS ASSIGNED LIKE BAD, LOW, ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional tool for this type of analysis is the `HAVING` clause. \n",
    "\n",
    "This clause can only be used with GROUP BY, never on its own. \n",
    "\n",
    "The reason is that HAVING puts a condition on partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- We are searching crowed regions\n",
    "\n",
    "WITH T AS (SELECT COUNT(*) AS TOTAL_ASIA FROM COUNTRY\n",
    "            WHERE CONTINENT='Asia')\n",
    "SELECT REGION, COUNT(*)/TOTAL_ASIA AS DISTRIBUTIONS\n",
    "FROM COUNTRY, T\n",
    "WHERE CONTINENT='Asia'\n",
    "GROUP BY REGION\n",
    "HAVING DISTRIBUTIONS>0.10;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want results that are ordered.\n",
    "\n",
    "For these cases, SQL has an additional clause, ORDER BY, which allows us to impose an order on the result of a query. \n",
    "\n",
    "The `ORDER BY`, if used, is added after any `WHERE` or `GROUP BY` clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT NAME, COUNT(LANGUAGE) AS SPOKEN_LANGUAGE\n",
    "FROM COUNTRYLANGUAGE, COUNTRY\n",
    "WHERE `Code` = CountryCode\n",
    "GROUP BY NAME\n",
    "HAVING SPOKEN_LANGUAGE>10\n",
    "ORDER BY SPOKEN_LANGUAGE DESC;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LIMIT n` means that the system will return at most n rows as answer.\n",
    "\n",
    "`LIMIT` is combined with `ORDER BY` to answer top k queries: these are questions when we want the most important k elements in the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TOP 5 REGIONS AREA\n",
    "SELECT REGION, SUM(SurfaceArea) as AREA\n",
    "FROM COUNTRY\n",
    "GROUP BY REGION\n",
    "ORDER BY AREA DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OFFSET` 20 means skip the first 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TOP 5 REGIONS AREA\n",
    "SELECT REGION, SUM(SurfaceArea) as AREA\n",
    "FROM COUNTRY\n",
    "GROUP BY REGION\n",
    "ORDER BY AREA DESC\n",
    "limit 5 OFFSET 20 ;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex Queries\n",
    "\n",
    "`WITH` clause is other way that one can get a subquery.\n",
    "\n",
    "It' is preferred by some programmers as it makes more clear that we are creating a temporary table for further computation.\n",
    "\n",
    "Another reason to use WITH is that it can be combined with subqueries in FROM clause for complex cases where several queries are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "WITH T AS (SELECT DATE(SALE_DATE) AS SALE_DATE, SUM(QUANTITY) AS SUM_QUANTITY\n",
    "\t\t\tFROM SALES_NORMALIZE\n",
    "\t\t\tGROUP BY SALE_DATE)\n",
    "SELECT SALE_DATE, AVG(SUM_QUANTITY) AS AVERAGE, STD(SUM_QUANTITY)\n",
    "FROM T\n",
    "GROUP BY SALE_DATE\n",
    "ORDER BY AVERAGE DESC;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop and Add a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- DROP A COLUMN IN A TABLE \n",
    "ALTER TABLE nameTable\n",
    "DROP COLUMN nameColumn;\n",
    "\n",
    "-- ADD A COLUMN IN A TABLE \n",
    "ALTER TABLE nameTable\n",
    "ADD nameColumn typeData AS values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "ALTER TABLE SALES_NORMALIZE\n",
    "ADD YEARS INTEGER AS (YEAR(SALE_DATE)),\n",
    "ADD MONHTS INTEGER AS (MONTH(SALE_DATE)),\n",
    "ADD DAYS INTEGER AS (DAY(SALE_DATE)),\n",
    "ADD HOURS INTEGER AS (HOUR(SALE_DATE)); \n",
    "\n",
    "-- ALTER TABLE SALES_NORMALIZE\n",
    "-- DROP YEARS,\n",
    "-- DROP MONHTS,\n",
    "-- DROP DAYS,\n",
    "-- DROP HOURS; "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down an answer into steps, writing a query for each step, and then combining the queries to obtain a final result is a good tactic when using SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "* *Proper data* We need to make sure that values are of the right kind and are in a proper format.  when we load data into the database we must make sure that values are read correctly. In spite of our efforts, we may end up with numbers that are read as strings (because of commas in the values, or other problems), or dates that are not recognized as such and also read as plain strings. Even if the data is read correctly, the format may\n",
    "be not appropriate for analysis.\n",
    "\n",
    "* *Missing values* refers exclusively to records that are present in the dataset but are not complete—in other words,\n",
    "missing attribute values. The general strategies are to ignore (delete) the affected data (either the attribute or the record), or to predict (also called impute) the absent values from other values of the same attribute or from value of other\n",
    "attributes. \n",
    "\n",
    "* *Outliers*. Outliers are data values that have characteristics that are very different from the characteristics of most other data values in the same attribute.\n",
    "\n",
    "* *Duplicate data.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "### Attribute transformations \n",
    "\n",
    "Attribute transformations  are operations that transform the values of an attribute to a certain format or range.\n",
    "\n",
    "* *Categorical attributes*: a typical transformation is normalization standardization.\n",
    "\n",
    "* For *numerical attributes*, typical transformations include scaling and normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Normalization** consists of making sure that all values are expressed using a\n",
    "known quantity as the unit. The typical example is the z-score, where values are\n",
    "expressed in terms of standard deviations from the mean:\n",
    "\n",
    "$$Z(v) = (x − μ)/ \\sigma$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization and the other tranformations we will do respect to `date(year, month, day)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE VIEW nameView as (SELECT * FROM tableName);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- CREATE A VIEW T\n",
    "CREATE VIEW T AS (SELECT YEARS, MONTHS, DAYS, HOURS, SUM(quantity) AS TOTAL \n",
    "\t\t\t\t  FROM SALES_NORMALIZE\n",
    "\t\t\t\t  GROUP BY YEARS, MONTHS, DAYS, HOURS);\n",
    "\n",
    "-- COMPUTE SUM OF QUANTITY AL LEVEL `YEARS, MONTHS, DAYS, HOURS`\n",
    "\n",
    "-- CREATE A VIEW T2\n",
    "CREATE VIEW T2 AS (SELECT YEARS AS YEARS_T2, MONTHS AS MONTHS_T2, DAYS AS DAYS_T2, \n",
    "\t\t\t\t\tAVG(TOTAL) AS AVERAGE, MAX(TOTAL) AS MAXIMUN, MIN(TOTAL) AS MINIMUN, STD(TOTAL) AS STANDAR\n",
    "\t\t\t\t\tFROM T\n",
    "\t\t\t\t\tGROUP BY YEARS, MONTHS, DAYS);\n",
    "\n",
    "-- COMPUTE MAX, MIN, AVG AND STD AT `YEARS, MONTHS, DAYS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- NORMALIZATION\n",
    "WITH TABLE_TOTAL AS (SELECT * \n",
    "\t\t\t\t\t FROM T, T2\n",
    "\t\t\t\t\t WHERE T.YEARS = T2.YEARS_T2 AND T.MONTHS = T2.MONTHS_T2 AND T.DAYS = T2.DAYS_T2)\n",
    "SELECT YEARS, MONTHS, DAYS, HOURS, TOTAL, (TOTAL - AVERAGE)/(STANDAR) AS Z\n",
    "FROM TABLE_TOTAL;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the distribution of the data is *skew* , that is, has a disproportionate effect on the tails of the distribution we apply:\n",
    "\n",
    "* $\\ln(x)$, $\\sqrt{x}, $ when $x>0$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling**\n",
    "* Nature *linear*: transforms all values to a number between $0$ and $1$:\n",
    "\n",
    "$$x \\leftarrow \\frac{x- \\bar{x}}{\\max(x) - \\min(x)}$$\n",
    "* Logistic scaling where value $x$ is mapped to \n",
    "\n",
    "$$x \\leftarrow \\frac{1}{1 + e^{-x}}$$\n",
    "  \n",
    "> The logistic function is used when the presence of outliers (very large or very small values) is suspected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ---------------\n",
    "-- LINEAR SCALING\n",
    "-- --------------- \n",
    "\n",
    "WITH TABLE_TOTAL AS (SELECT * \n",
    "\t\t\t\t\t FROM T, T2\n",
    "\t\t\t\t\t WHERE T.YEARS = T2.YEARS_T2 AND T.MONTHS = T2.MONTHS_T2 AND T.DAYS = T2.DAYS_T2)\n",
    "SELECT YEARS, MONTHS, DAYS, HOURS, TOTAL, (TOTAL - AVERAGE)/(MAXIMUN - MINIMUN) AS MAXMIN\n",
    "FROM TABLE_TOTAL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- CREATE A TABLE NORMALIZE\n",
    "CREATE VIEW NORMAL AS \n",
    "WITH TABLE_TOTAL AS (SELECT * \n",
    "\t\t\t\t\t FROM T, T2\n",
    "\t\t\t\t\t WHERE T.YEARS = T2.YEARS_T2 AND T.MONTHS = T2.MONTHS_T2 AND T.DAYS = T2.DAYS_T2)\n",
    "SELECT YEARS, MONTHS, DAYS, HOURS, TOTAL, (TOTAL - AVERAGE)/(STANDAR) AS Z\n",
    "FROM TABLE_TOTAL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT 1/(1 + EXP(-Z))\n",
    "FROM NORMAL;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Strings\n",
    "\n",
    "Cleaning categorical attributes means making sure that one and only\n",
    "one name (string) represents each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE STUDENT(\n",
    "    StudentId INT,\n",
    "    Department CHAR(45)\n",
    ")\n",
    "\n",
    "INSERT INTO STUDENT VALUES\n",
    "(1,\"Dept of Economics\"),\n",
    "(2,\"Econ\"),\n",
    "(3, \"Department of Econ\");\n",
    "\n",
    "UPDATE STUDENT\n",
    "SET Department = \"Economics\"\n",
    "WHERE Department REGEXP '.*Econ.*'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Functions that clean the string: they change it by getting rid of certain characters or transforming existing characters. Among them are `TRIM()`, `LOWER()`, and `UPPER()`. The inverse of this (adding characters to a string) is called padding,\n",
    "expressed with `LPAD()`, `RPAD()`, and others.\n",
    "\n",
    "* Functions that find elements (characters or substrings) within a given string. The most popular ones are `POSITION()` and `STRPOS()`.\n",
    "\n",
    "* Functions that extract elements of a string or split a string into parts. This includes functions `SUBSTR()` and `SPLIT()`. The inverse of this (putting together several strings into a single one) is usually called concatenation, expressed by `CONCAT()`.\n",
    "\n",
    "* `LENGTH()`, which returns the number of characters in a string.\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- LENGHT\n",
    "SET @VARIABLE = \"MACROECONOMICS\";\n",
    "SELECT LENGTH(@VARIABLE);\n",
    "\n",
    "-- LPAD\n",
    "-- If VARIABLE HAS A LENGHT EQUAL OR LESS 20 SPACES, THEN * IS FILLED AT LEFT\n",
    "SELECT LPAD(@VARIABLE, 20, \"*\");\n",
    "\n",
    "-- POSITION\n",
    "-- TO FIND THE FIRST POSITION OF THE PIECE OF TEXT\n",
    "SET @VARIABLE = \"1 FORM\";\n",
    "SELECT POSITION(\"FORM\" IN @VARIABLE);\n",
    "\n",
    "-- UPPER AND LOWER\n",
    "SET @MYNAME = \"Jeff\";\n",
    "SELECT UPPER(@MYNAME );\n",
    "\n",
    "SET @MYNAME = \"Jeff\";\n",
    "SELECT LOWER(@MYNAME );\n",
    "\n",
    "-- SUBSTRING\n",
    "-- EXTRACT A SUBSTRING FROM A STRING (START AT POSITION 1 (POSITION WHEN IS FOUND \"AV\"), EXTRACT 5 CHARACTERS):\n",
    "SET @VARIABLE = \"Avenger - Ultron Age\";\n",
    "SELECT SUBSTR(@VARIABLE, POSITION(\"Av\" IN @VARIABLE), 7);\n",
    "\n",
    "-- 1 POINT OUT THE TIMES IN WHICH MUST APPEAR THE \".\"\n",
    "-- THE SIGN MINUS (-) START THE COUNTING FROM RIGHT\n",
    "-- THE MEANING IS: \n",
    "-- >EXTRACT ALL STRING FROM THE LEFT UNTIL 1 TIME THE '.' APPEAR. HER THE OUTPUE IS `com`\n",
    "SELECT SUBSTRING_INDEX(\"www.google.com\", '.', -1);\n",
    "\n",
    "-- CONCAT STRING `CONCAT(STRING1, STRING2 , ...)`\n",
    "SELECT CONCAT('10' ,'-05-', '2020');\n",
    "\n",
    "-- EXTRACT 5 CHARS STARTING AT RIGHT. THE YEAR\n",
    "SELECT RIGHT(\"10-05-2020\", 4);\n",
    "\n",
    "-- EXTRACT 5 CHARS STARTING AT LEFT. THE DAY\n",
    "SELECT LEFT(\"10-05-2020\", 2);\n",
    "\n",
    "\n",
    "-- CREATE A NEW COLUMN `PRODUCT_ID_STR`\n",
    "ALTER TABLE SALES_NORMALIZE\n",
    "ADD COLUMN PRODUCT_ID_STR  CHAR(10) AS (CAST(PRODUCT_ID AS CHAR(10)));\n",
    "\n",
    "-- GROUP_CONCAT\n",
    "SELECT YEARS, MONTHS, DAYS, HOURS, GROUP_CONCAT(PRODUCT_ID_STR)\n",
    "FROM SALES_NORMALIZE\n",
    "GROUP BY YEARS, MONTHS, DAYS, HOURS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Dates\n",
    "\n",
    "* The SQL standard form for `DATE` is defined as `YYYY-MM-DD`. The reason to adapt this format is that, if sorted, it\n",
    "yields chronological order.\n",
    "\n",
    "* The standard for `TIME` is `HH:MM:SS`[.NNNNNNN] (two digits for hours first, followed by two for minutes, followed by two digits for seconds, and optionally seven digits for fractions of a second).\n",
    "\n",
    "* The SQL standard for `DATETIME` (timestamps) is a `DATE` plus a TIME, separated by a space: `YYYY-MM-DD HH:MM:SS`.\n",
    "* Some functions:\n",
    "    * `current_time()`\n",
    "    * `current_date()`\n",
    "    * `now()` is equivalent to `current_datetime()`\n",
    "\n",
    "\n",
    "* [MySQL Date and Time functions ->  mysql.com](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- SELECT THE date\n",
    "SELECT DATE('2020-12-20 22:12:32');\n",
    "\n",
    "-- SELECT THE time\n",
    "SELECT TIME('2020-12-20 22:12:32');\n",
    "\n",
    "-- add 4 days. Can be other itervals like months, year\n",
    "SELECT addate('2020-12-20 22:12:32', interval 4 day);\n",
    "\n",
    "-- diff between dates\n",
    "SELECT datediff('2021-12-20', '2020-12-20');\n",
    "\n",
    "-- get a format of date\n",
    "SELECT date_format('2021-12-20', '%a-%d-%b-%Y') ;\n",
    "\n",
    "-- more about date -> `https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html#function_date-format`\n",
    "\n",
    "-- `MAKEDATE(year,dayofyear)`\n",
    "SELECT MAKEDATE(2015, 90);\n",
    "\n",
    "-- `str_to_date`\n",
    "set @mydate = \"15-04-2015\";\n",
    "SELECT str_to_date(@mydate, '%d-%m-%Y');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that SQL’s `Null` is not a value, but an indicator that there is a `hole` in a tuple, i.e. a Null denotes the absence of a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "USE LINIO_SAMPLE;\n",
    "\n",
    "SELECT COUNT(*)  AS CURRENT_NULL FROM PRODUCTS\n",
    "WHERE CURRENT_PRICES IS NULL;\n",
    "\n",
    "SELECT COUNT(*)  AS CURRENT_NOT_NULL FROM PRODUCTS\n",
    "WHERE CURRENT_PRICES IS NOT NULL;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule, aggregates ignore `Nulls`, with the exceptions `count`\n",
    "\n",
    "* `SUM`, `AVG`, `MAX`, `MIN`\n",
    "\n",
    "When operating in the context of a `GROUP BY` aggregates ignore Nulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SET SQL_SAFE_UPDATES = 0;\n",
    "ALTER TABLE PRODUCTS ADD COLUMN  CLASSIFICATION CHAR(12);\n",
    "-- ALTER TABLE PRODUCTS DROP COLUMN  CLASSIFICATION;\n",
    "\n",
    "UPDATE PRODUCTS\n",
    "SET CLASSIFICATION = (CASE \n",
    "\t\t\tWHEN CURRENT_PRICES < 30 THEN \"Low\" \n",
    "\t\t\tWHEN CURRENT_PRICES < 50 THEN \"Medium\"\n",
    "            ELSE \"High\" END);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ADD WRONG COLUMN\n",
    "ALTER TABLE PRODUCTS ADD COLUMN  WRONG FLOAT;\n",
    "UPDATE PRODUCTS\n",
    "SET WRONG = (CASE \n",
    "\t\t\tWHEN CURRENT_PRICES BETWEEN 45 AND 55 THEN NULL\n",
    "            ELSE CURRENT_PRICES END);\n",
    "\n",
    "SELECT \n",
    "    CLASSIFICATION, \n",
    "    COUNT(*) AS COUNT_TOTAL, \n",
    "    SUM(CASE WHEN WRONG IS NULL THEN 1 ELSE 0 END) AS COUNT_NULL,\n",
    "    SUM(CASE WHEN WRONG IS NOT NULL THEN 1 ELSE 0 END) AS COUNT_NOT_NULL\n",
    "FROM PRODUCTS\n",
    "GROUP BY CLASSIFICATION;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once nulls are detected and we know which attribute (or attributes) are at fault, both operations are quite easy in SQL—the first one calls for a command like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- DELETE THE OBSERVATION FROM tableName IF columnName IS NULL\n",
    "DELETE FROM tableName\n",
    "WHERE columnName IS NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- FILL THE SPACES NULL WITH ZERO\n",
    "SELECT COALESCE(WRONG, 0)\n",
    "FROM PRODUCTS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- THE NULLS IN STRING ARE \"\"\n",
    "-- ONE MAY THAT WE CAN FIND THEM IS WITH LENGHT\n",
    "\n",
    "SELECT COUNT(columnString) FROM tableName WHERE LENGTH(columnString) = 0;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection\n",
    "\n",
    "An outlier is a value that is not `normal`, in the sense that it is quite different from other values in a domain. It tends to be extreme (for numerical attributes) and unusual (for categorical attributes). \n",
    "\n",
    "* If the value is an error, then we can assing a value normal.\n",
    "* If the value is a real value, then it is a useful information. \n",
    "\n",
    "We can use a confidence of the interval in order to see them or use MAD (Median Absolute Deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "WITH T3  AS (SELECT STD(TOTAL) AS STD_TOTAL, \n",
    "\t\t\t\t\tAVG(TOTAL) AS AVG_TOTAL \n",
    "\t\t\tFROM T\n",
    "\t\t\tWHERE HOURS = 9)\n",
    "SELECT *\n",
    "FROM T,T3\n",
    "WHERE HOURS = 9 AND (TOTAL > AVG_TOTAL + 2*STD_TOTAL  OR TOTAL < AVG_TOTAL - 2*STD_TOTAL);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Detection and Removal\n",
    "\n",
    "The task of examining two or more rows and determining whether they refer to the same observation, object, or entity in the real world.\n",
    "\n",
    "This task is know as *data linkage*\n",
    "\n",
    "Identification\n",
    "\n",
    "* If the *Id* is the same and the remaining features are the same simultaneuly. (This is not useful qhen we create artificially the *Id*)\n",
    "\n",
    "* If two values are very close, we can consider them the same for the purposes of duplicate detection. In this sense we need a measurement of distance, small distance point out the values are similar.\n",
    "\n",
    "* There are some funtions for string, but for numerical values we have to define the distance.\n",
    "  * Distances are comparable across domains.\n",
    "  * The *denominator* is the largest possible distance between two items and serves to normalize the result\n",
    "\n",
    "\n",
    "$$\\text{dist}(r, s) = \\sum_{k=1}^{n} \\frac{ \\text{dist}_k(r.A_k, s.A_k )}{\\max{\\text{dist}_k}}$$\n",
    "\n",
    "* If we have the weights we can use to compute the *distance*, $\\text{dist}(r, s) = \\sum_{k=1}^{n} \\frac{ w_k\\text{dist}_k(r.A_k, s.A_k )}{\\max{\\text{dist}_k}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE DATABASE STUDENTS;\n",
    "\n",
    "-- DROP TABLE table_student;\n",
    "-- DROP DATABASE STUDENT;\n",
    "\n",
    "CREATE TABLE STUDENT(\n",
    "    first_name char(32),\n",
    "    last_name char(32),\n",
    "    address_student char(100));\n",
    "\n",
    "INSERT INTO STUDENT VALUES\n",
    "('Badfinger', 'British', 'Av. San Martin de Porres 906'),\n",
    "('Badfinger', 'British', 'Av. San Martin de Porres 906'),\n",
    "('Dante', 'Toribio', 'Av. Tacna 784'),\n",
    "('Maria', 'Mauricia', 'Jr. La Union');\n",
    "\n",
    "-- To identify duplicates values\n",
    "\n",
    "SELECT COUNT(*) as count_duplicates, first_name, last_name, address_student\n",
    "FROM STUDENT\n",
    "GROUP BY first_name, last_name, address_student\n",
    "HAVING count_duplicates>1;\n",
    "\n",
    "-- To identify duplicates values string or how far apart they are\n",
    "SELECT *\n",
    "FROM STUDENT\n",
    "WHERE first_name REGEXP '^[Bb]ad.*';"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "* When we detect similarity values we can know the inconsistences in the values. Maybe two values can have two different *ages*. What is the True?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal:\n",
    "* Find the right format to algorithms or analysis.\n",
    "* Typical operations used at this stage include *aggregation*, *sampling*, *dimensionality reduction*, *feature creation*, *discretization*, and *binarization*\n",
    "\n",
    "  * *agg*: This operation is compute by *group by*. In this process we change the *level of granularity*. Instead we work with day period, we work with week.\n",
    "\n",
    "  * *sampling*: This is carry out when we have a large data set.\n",
    "  * *Discretization*: his is a transformation that changes numerical continuous values to ordinal ones.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- SAMPLING. THE SIZE OF THE SAMPLE IS 50. \n",
    "SELECT * FROM PRODUCTS ORDER BY RAND() LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- DISCRETIZATION\n",
    "SET SQL_SAFE_UPDATES = 0;\n",
    "ALTER TABLE PRODUCTS ADD COLUMN  CLASSIFICATION CHAR(12);\n",
    "-- ALTER TABLE PRODUCTS DROP COLUMN  CLASSIFICATION;\n",
    "\n",
    "UPDATE PRODUCTS\n",
    "SET CLASSIFICATION = (CASE \n",
    "\t\t\tWHEN CURRENT_PRICES < 30 THEN \"Low\" \n",
    "\t\t\tWHEN CURRENT_PRICES < 50 THEN \"Medium\"\n",
    "            ELSE \"High\" END);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "**Restructuring Data**\n",
    "\n",
    "* This is neccessary when the data is not *tidy*.  since most Data Mining and Machine Learning tools assume that all the data is presented as a single tabular structure.\n",
    "\n",
    "* Applying *pivot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE Earthquakes(\n",
    "    Magnitude FLOAT, \n",
    "    Y2000 INT,\n",
    "    Y2001 INT,\n",
    "    Y2002 INT,\n",
    "    Y2003 INT\n",
    ");\n",
    "\n",
    "INSERT INTO Earthquakes VALUES \n",
    "(4.8, 1, 2, 3,6),\n",
    "(3.1, 3, 0, 5, 1),\n",
    "(2.9, 2, 0, 0, 4),\n",
    "(5, 1, 1, 2, 1)\n",
    "\n",
    "\n",
    "CREATE TEMPORARY table tmp(year_n INT);\n",
    "\n",
    "INSERT INTO tmp VALUES (2000), (2001), (2002), (2003);\n",
    "\n",
    "CREATE TABLE earthquatesTidy AS\n",
    "(SELECT Magnitude, year_n, sum(\n",
    "    CASE WHEN year_n = 2000 THEN Y2000\n",
    "    WHEN year_n = 2001 THEN Y2001\n",
    "    WHEN year_n = 2002 THEN Y2002\n",
    "    WHEN year_n = 2003 THEN Y2003\n",
    "    ELSE 0 END) as numberquakes\n",
    "FROM Earthquakes, tmp\n",
    "GROUP BY Magnitude, year_n);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work?\n",
    "<img src=\".\\assets\\img\\unpivot.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- CREATE A NEW TABLE BASED ONE ORIGINAL\n",
    "CREATE TABLE new_tbl LIKE orig_tbl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- To recover what we had above\n",
    "SELECT \n",
    "Magnitude,\n",
    "sum(CASE WHEN year_n = 2000 THEN numberquakes ELSE 0 END) as Y2000,\n",
    "sum(CASE WHEN year_n = 2001 THEN numberquakes ELSE 0 END) as Y2001,\n",
    "sum(CASE WHEN year_n = 2002 THEN numberquakes ELSE 0 END) as Y2002,\n",
    "sum(CASE WHEN year_n = 2003 THEN numberquakes ELSE 0 END) as Y2003\n",
    "FROM earthquatesTidy\n",
    "GROUP BY magnitude;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Dummy Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE SIMPLE_TAB (\n",
    "    NAME_TAB CHAR(50),\n",
    "    CATEGORY CHAR(10)\n",
    ");\n",
    "\n",
    "INSERT INTO SIMPLE_TAB VALUES\n",
    "('Jones', 'A'),\n",
    "('Jones', 'C'),\n",
    "('Smith', 'B'),\n",
    "('Lewis', 'B'),\n",
    "('Lewis', 'C');\n",
    "\n",
    "SELECT NAME_TAB,\n",
    "sum(CASE WHEN CATEGORY = \"A\" then 1 ELSE 0 END) AS A,\n",
    "sum(CASE WHEN CATEGORY = \"B\" then 1 ELSE 0 END) AS B,\n",
    "sum(CASE WHEN CATEGORY = \"C\" then 1 ELSE 0 END) AS C\n",
    "FROM SIMPLE_TAB\n",
    "GROUP BY NAME_TAB;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata and Implementing Workflows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sometimes modify the original, raw data, generating new datasets. \n",
    "\n",
    "We need to manage all these datasets and keep track of how they were created and why. The step-by-step transformation of the data is sometimes called a `workflow`, as it corresponds to a sequence of actions applied to the data or to the results of earlier actions.\n",
    "\n",
    "* Most actions can be done in two modes: *destructive* and *non-destructive*:\n",
    "\n",
    "  * *Destructive mode*, the action changes some data, so that the old version of it is lost, and a new version put in its place. \n",
    "  \n",
    "  * In *non-destructive mode*, data is transformed by generating a new version, but keeping the old data (making a copy). \n",
    "\n",
    "* But an action can be classified as *reversible* or *non-reversible*:\n",
    "  \n",
    "  * A *reversible action* After concatenating two strings using some character c as separator, we  can split by the character c and go back to original variables.\n",
    "    \n",
    "  * A *non-reversible action*, in contrast, cannot be undone: for instance, trimming whitespaces from a string cannot be undone unless we note, for each string, how many whitespaces were deleted, and where they appeared—without this, we cannot recreate the original string from the modified one.\n",
    "\n",
    "* If we implemented a *non-destrctive mode* the *non-reversible action* can be reversed, since we have a copy of the old data. In this case we must care about the *extra storage*.\n",
    "\n",
    "* In databases, a destructive action is implemented by an `UPDATE` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TEMPORARY table tmp(\n",
    "IND INT,\n",
    "BOOK TEXT,\n",
    "AUTHOR TEXT,\n",
    "PUBLISHING TEXT,\n",
    "PAG INT);\n",
    "-- DROP TEMPORARY table  TMP;\t\n",
    "SELECT * FROM TMP;\n",
    "\n",
    "INSERT INTO TMP VALUES\n",
    "(1, \"Harry Potter\", \"J.K.Rowing\", \"Bloomsbury Publishing\", 500),\n",
    "(2, \"La tentacion del fracaso\", \"Juio Ramón Ribeyro\",\"Planeta\", 704),\n",
    "(3 , \"Cien años de soledad\", \"Gabriel Garcia Marquez\", \"Sudamericana\", 496);\n",
    "\n",
    "UPDATE TMP SET BOOK = UPPER(BOOK);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is called an in-place update because the space used by the attribute is reused\n",
    "for the new value; the old value is gone.\n",
    "\n",
    "* In contrast, a *non-destructive action* is implemented by adding the result of a change as a new attribute in the table, or creating a brand new table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Add new column in table\n",
    "\n",
    "ALTER TABLE books ADD column autor_num_pages CHAR(120);\n",
    "\n",
    "ALTER TABLE books MODIFY num_pages CHAR(20);\n",
    "\n",
    "UPDATE books\n",
    "SET autor_num_pages = CONCAT(title,\"_\" , num_pages);\n",
    "\n",
    "-- CREATE A NEW TABLE THAT INCLUDE OUR NEW COLUMN\n",
    "CREATE TEMPORARY TABLE TMP1 AS\n",
    "SELECT IND, BOOK, AUTHOR, PUBLISHING, PAG, CONCAT(BOOK,\"-\" , AUTHOR) as BOOK_AUTHOR\n",
    "FROM TMP;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `view` is a virtual table, one that is defined through a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "USE LINIO_SAMPLE;\n",
    "CREATE VIEW VIEW1 AS\n",
    "(SELECT *\n",
    "FROM PRODUCTS);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The view’s schema is defined, implicitly, by the query used. The view inherits the attribute names from the table used. If new names are desired, they can be specified. And we can pass some conditions.\n",
    "\n",
    "The view does not actually have data of its own. \n",
    "\n",
    "If we want to see what is in a view, the system simply runs the query that was used in the view definition. As a result, if the tables used in the query that defined the view change, the view itself changes accordingly. \n",
    "\n",
    "Note that, since the view is not actually stored, no extra space is needed for views. On top of that, we can\n",
    "define views using already-defined views, so that this approach can be used for complex workflows. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "## Metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "*Data about data*\n",
    "\n",
    "Having metadata is highly beneficial:\n",
    "\n",
    "* To operate meaningfully in data, we must only carry out operations that make sense for that data: it makes no sense to add a temperature and a height\n",
    "\n",
    "* Having metadata can guide our decision as to how to clean and preprocess the data.\n",
    "\n",
    "* Sometimes data analysis will reveal that a transformation was not quite appropriate; we may want to undo the effects of the change and perhaps try something different.\n",
    "\n",
    "* If we need to share/export our data, we need to explain what the data are for others to be able to use them.\n",
    "\n",
    "* Very frequently data collected for a certain purpose is reused later for different projects, with different goals. Hence, it is important to understand what the data was originally meant to represent, and how it has been transformed.\n",
    "\n",
    "* It provides a trace of how data came to be, metadata is crucial in helping with repeatability of analysis in order to generate reproducible results.\n",
    "\n",
    "* There are many diverse classifications of metadata in the literature:\n",
    "\n",
    "  * *Structural metadata*: for both structured and semistructured data, metadata should include the schema or a given dataset and also a description and classification of the domain of each attribute in the schema.\n",
    "\n",
    "  * *domain metadata*: \n",
    "    \n",
    "    * Syntactic: Type of value is the data (numerical or categorical). \n",
    "    \n",
    "      For **numerical**: *range* of the values and values tiptical, the *unit*, *precision* and *scale*. \n",
    "\n",
    "      e.g. `85` given its units: `thousand` and scale `Canadian dollars` means `CAN $85,000`\n",
    "\n",
    "      For **categorical**: the *pattern*.\n",
    "\n",
    "      e.g. `2020-10-09` given the pattern `year-month-day` we don't confuce between october 9th or september 10th.\n",
    "\n",
    "\n",
    "    * Semantic:  refers to what the data is supposed to represent.\n",
    "\n",
    "      e.g. price:  may give the price of a product, but this may be before or after taxes, with or without discounts. Here we must define the attribute.\n",
    "\n",
    "      e.g. FY15: can be refer `Females Younger than 15` or `Fiscal Year 2015`.So a good definition is valuable\n",
    "\n",
    "      e.g. 1-5 customer satisfaction. We hahe to know if the heights value is the more satisfaction.\n",
    "\n",
    "    * Provenance/lineage metadata: this describes where data comes from.\n",
    "\n",
    "      * *Provenance* \n",
    "      \n",
    "        * In the first sense is a way to explain how data was obtained. It should describe the source (for instance, a sensor, a web page, or a file) and the date when data was obtained. It is useful to list constraints and assumptions under which the data was generated.\n",
    "\n",
    "        * The second sense of provenance applies to datasets generated by manipulating data through the data life cycle process: by cleaning, pre-processing, analyzing, etc. If we fill the null values by mean, we must document it.\n",
    "\n",
    "    * Quality metadata: this is a description of how good the data is. This metadata may not be available at all when data is acquired and may have to be generated after the EDA step. Nevertheless, it is an important aspect to document, since many datasets are found to contain problems that affect their usefulness. \n",
    "\n",
    "      * *Accuracy*: how close the represented value is to the actual value.\n",
    "\n",
    "      * *Completeness*: In a relational database, we can talk of `schema completeness` (do we have all entities and attributes needed?), `column completeness` (how many values are missing in an attribute?), and `population completeness` (do we have all values from the domain?).The latter is especially important since many datasets are samples from an underlying population.\n",
    "\n",
    "      * *Consistency*: this measures whether the data as a whole is sound; it answers the question: are there contradictions in or across records? Sometimes a record may contain inconsistencies; for example, a person record with a name like `Jim Jones` and `female` for `sex`.\n",
    "\n",
    "      * *Timeliness*: this refers to value changes along time. We want to know, mainly, two things: how often data changes ( volatility) and how long ago was data created and/or acquired (currency); the two of them together determine whether the data is still valid.\n",
    "\n",
    "* When is metadata generated? Ideally, metadata should be created at the acquisition/storage stage, when data is first acquired. But if this data don't have, we can generate one, after EDA.\n",
    "\n",
    "      \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can create a table with the following schema:`(attribute-name, representation, domain, provenance, accuracy, completeness, consistency, currency, precision, certainty)`\n",
    "\n",
    "* Each time an action is carried out, the effect should be registered. Thus, a second table for each dataset should be added to describe actions. For each action, we want to register\n",
    "\n",
    "  * the attribute or attributes affected by the action;\n",
    "  * the change made (and any functions applied);\n",
    "  * the values of any parameters used by the functions;\n",
    "  * when the action was carried out;\n",
    "  * why it was carried out;\n",
    "  * who carried the action out.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Joins\n",
    "\n",
    "`join` by default has the `inner` behaviour, so, that mean if there are values that don't match, those are deleted. This effect is called *existecial effect*.\n",
    "\n",
    "Sometimes this is useful, but not always.\n",
    "\n",
    "The table sales record the sales of the employees that sales some product. If the the employee din't go to work this will never appear in the tables sales, but in the table emp will.\n",
    "\n",
    "We want to measure the performance of the employees, specially, of the employees with bottom performance. This can be achieve adding the sales make by each employee.\n",
    "\n",
    "As we can said `join` by default has the `inner` behaviour, so if we use this the results will be missing. Here won't appear eth employees that will be sick.\n",
    "\n",
    "So we need to add `left outer` to command `join`, this keeping the values of the all employees althought they don't sell anything"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we join tables we must analyze the effects of using `inner`, `outer`, `right` or `left`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE DATABASE IF NOT EXISTS SOUL;\n",
    "USE SOUL;\n",
    "-- SHOW TABLES FROM SOUL;\n",
    "-- DROP DATABASE SOUL;\n",
    "-- DROP TABLE SALES;\n",
    "\n",
    "\n",
    "SELECT IDX, SUM(SALES) AS SALES_EMP\n",
    "FROM EMPLOYEES LEFT OUTER JOIN SALES ON (SALES.IDX_EMP = EMPLOYEES.IDX)\n",
    "GROUP BY IDX ; "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*multiplicative effect*\n",
    "\n",
    "When a tuple in table R matches several tuples in table S, this tuple gets repeated as many times as there are matches.\n",
    "\n",
    "<div align = \"center\">\n",
    "    <img src= \"./assets/img/multiplicative-effects.png\" />\n",
    "    <img src= \"./assets/img/multiplicative-effects-1.png\" />\n",
    "<div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must try to avoid this joining. The solution is join by parts, `branch` and `loan` by one hand, and `brach` and `account` by the other hand.\n",
    "\n",
    "In each join we do the calculation that we can do."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex subqueries\n",
    "\n",
    "* Aggregated subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "WITH T3  AS (SELECT YEARS AS YEARS_T3, MONTHS AS MONTHS_T3, DAYS AS DAYS_T3, AVG(TOTAL) AS AVERAGE\n",
    "\t\t\t FROM T\n",
    "             GROUP BY YEARS, MONTHS, DAYS)\n",
    "SELECT YEARS, MONTHS, DAYS, HOURS, TOTAL,AVERAGE, TOTAL > AVERAGE AS GT\n",
    "FROM T JOIN T3 ON (T.YEARS = T3.YEARS_T3 && T.MONTHS = T3.MONTHS_T3 && T.DAYS = T3.DAYS_T3);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Subqueries with (NOT) IN\n",
    "\n",
    "Employees that didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM SALES_NORMALIZE\n",
    "WHERE PRODUCT_ID IN (SELECT PRODUCT_ID\n",
    "\t\t\t\t\t FROM PRODUCTS\n",
    "\t\t\t\t\t WHERE CURRENT_PRICES>50);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the sales whose prices of products are greater than 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Subqueries with (NOT) EXISTS\n",
    "\n",
    "EXISTS is satisfied if the subquery answer is not empty. In this case, there aren't products with prices greater than 60, so the query don't return any records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(*)\n",
    "FROM  SALES_NORMALIZE\n",
    "WHERE PRODUCT_ID AND EXISTS(SELECT PRODUCT_ID\n",
    "\t\t\t\t\t\t\tFROM PRODUCTS\n",
    "\t\t\t\t\t\t\tWHERE CURRENT_PRICES>60);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Subqueries with ANY, ALL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- BOTH RETURN THE SAME RESULTS\n",
    "\n",
    "SELECT *\n",
    "FROM  SALES_NORMALIZES\n",
    "WHERE SALE_PRICE >= ANY(SELECT MAX(CURRENT_PRICES) FROM PRODUCTS);\n",
    "\n",
    "\n",
    "SELECT COUNT(*)\n",
    "FROM  SALES_NORMALIZE\n",
    "WHERE SALE_PRICE >= (SELECT MAX(CURRENT_PRICES) FROM PRODUCTS);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "## Windows and Window Aggregates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windows have been added to the SQL standard to give more flexibility than `GROUP BY` allows\n",
    "\n",
    "A *window* is a set of rows from a table, specified by the user, on which certain calculations are performed, but the difference between those is the window are no collapsed to row, it's possible to operate within them. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax:\n",
    "\n",
    "`WINDOW name AS (PARTITION-BY ...ORDER-BY ...FRAME ...)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three commponents are optional\n",
    "* PARTITION BY: take a list of features or arguments and put in a window all rows that have the same features\n",
    "* ORDER BY: sort the values within a windows\n",
    "* FRAME: it is a subset of rows within window to which the aggregation function is applied. If we don't especify, it takes all rows in each window."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the order within a window we can say it is the \"first\", this is \"last\", and this iss \"current\" row (this is used like reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- 1. MOVING AVERAGE OVER A WINDOW OF 3 ROWS, THE CURRENT AND THE 2 PRECEDING\n",
    "WITH T AS (SELECT YEARS, MONTHS, DAYS, SUM(quantity*sale_price) AS amount\n",
    "\t\t\t\tFROM metro_sample.sales\n",
    "\t\t\t\tGROUP BY YEARS, MONTHS, DAYS)\n",
    "\n",
    "SELECT YEARS, MONTHS, DAYS, AVG(amount) OVER W AS MA_OVER_AMOUNT\n",
    "FROM T\n",
    "WINDOW W AS (PARTITION BY YEARS, MONTHS ORDER BY DAYS ROWS 2 PRECEDING);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partition is done by (YEARS, MONTHS), each partition is ordered by DAYS. Then we use the the current record and the 2 preceding to compute the average.\n",
    "\n",
    "This is moving average by 3 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "WITH T AS (SELECT YEARS, MONTHS, DAYS, SUM(quantity*sale_price) AS amount\n",
    "\t\t\t\tFROM metro_sample.sales\n",
    "\t\t\t\tGROUP BY YEARS, MONTHS, DAYS)\n",
    "\n",
    "SELECT YEARS, MONTHS, DAYS, SUM(amount) OVER W AS MA_OVER_AMOUNT\n",
    "FROM T\n",
    "WINDOW W AS (PARTITION BY YEARS, MONTHS ORDER BY DAYS  ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code compute the cumulative sum in each window. The cumulative sum is done by `DAYS`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple windows** can be defined in the same query\n",
    "\n",
    "This compute the quantity by all, grouping by seller_id and grouping by (year, month, day)\n",
    "\n",
    "It doesn't work in MySql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT product_id, quantity, \n",
    "\tAVG(quantity) over W AS mean_overall,\n",
    "\tAVG(quantity) over W1 AS mean_by_product_id,\n",
    " \tAVG(quantity) over W2 AS mean_by_year_month_day\n",
    "FROM metro_sample.sales\n",
    "WINDOW \n",
    "\tW AS (), -- WINDOW THAT TAKE ALL TABLE\n",
    "\tW1 AS (PARTITION BY product_id), -- WINDOW COMPUTE THE AVG IN EACH PARTITION PRODUCT_ID\n",
    "\tW2 AS (PARTITION BY YEARS, MONTHS, DAYS) --WINDOW THAT COMPUTE THE AVG IN EACH PARTITON (YEAR, MONTHS, DAYS) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **reuse a window**\n",
    "\n",
    "If we don't specify the number of rows that can take to comptute the aggregation, by default it uses all the window. This happen in `W`.\n",
    "\n",
    "Then we reuse this windows `W` to compute cummulative sum by hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT YEARS, MONTHS, DAYS, HOURS, quantity,\n",
    "    AVG(quantity) over W,\n",
    "    SUM(quantity) over W1 AS CUMSUM\n",
    "FROM SALES_NORMALIZE\n",
    "WINDOW \n",
    "    W AS (PARTITION BY YEARS, MONTHS, DAYS ORDER BY YEARS, MONTHS, DAYS), \n",
    "    -- HERE THE FRAME IS THE WINDOW GENERATED BY PARTITION \n",
    "    W1 AS (W ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW); \n",
    "    -- HERE THE FRAME IS DYNAMIC, TAKE ALL ROWS PRECEDING TO CURRENT ROW AND THE ROW ITSELF. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RANKING**\n",
    "\n",
    "Given the partition and the `ORDER` passed in the window, we rank over total. Create a new column called RANKING that start at 1 and end in the lenght of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "WITH T2 AS (WITH T AS (SELECT years, user_id, SUM(quantity) AS total\n",
    "\t\t\t\t\t\tFROM metro_sample.sales\n",
    "\t\t\t\t\t\tGROUP BY years, user_id)\n",
    "\t\n",
    "\t\t\tSELECT YEARS, user_id,total, RANK() over W AS ranking\n",
    "\t\t\tFROM T\n",
    "\t\t\tWINDOW W AS (partition by YEARS ORDER BY total DESC ))\n",
    "SELECT *\n",
    "FROM T2\n",
    "WHERE ranking <= 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "Top 5 of user by year that bought us products"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with this, SQL distinguishes between `RANK()` and `DENSE_RANK()`, which does not skip ranks. As a simple example, if in some stores the top 4 sales are 1000, 900, 900, and 800, `RANK()` will produce 1, 2, 2, 4 and `DENSE_RANK()` will produce 1, 2, 2, 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "WITH W1 AS (SELECT PERIODO, CODIGO_ALUMNO, CODIGO_DE_CURSO, RANK() over W AS ranking\n",
    "            FROM OCURRENCIA\n",
    "            WINDOW W AS (partition by PERIODO, CODIGO_ALUMNO, CODIGO_DE_CURSO))\n",
    "SELECT * \n",
    "FROM W1\n",
    "WHERE ranking=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "**Other Functions**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FIRST_VALUE`: Returns the value of expr from the first row of the window frame.\n",
    "\n",
    "The `LAST_VALUE`: Returns the value of expr from the last row of the window frame\n",
    "\n",
    "The `LAG()` (and the similar `LEAD()` function) are often used to compute differences between rows. Compuet lag at level partition.\n",
    "\n",
    "The `ROW_NUMBER()`: Returns the number of the current row within its partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "WITH T2 AS (WITH T AS (SELECT YEARS, MONTHS, DAYS, SUM(quantity) AS total\n",
    "\t\t\t\t\t\tFROM metro_sample.sales\n",
    "\t\t\t\t\t\tGROUP BY YEARS, MONTHS, DAYS)\n",
    "\t\t\tSELECT YEARS, MONTHS, DAYS, total,\n",
    "\t\t\t\tlag(total) OVER W AS total_lag_1,\n",
    "\t\t\t\tROW_NUMBER() over W AS `row_number`\n",
    "\t\t\tFROM T\n",
    "\t\t\tWINDOW W AS (PARTITION BY YEARS, MONTHS ORDER BY YEARS, MONTHS, DAYS))\n",
    "SELECT YEARS, MONTHS, DAYS, total - total_lag_1 AS diff\n",
    "FROM T2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This compute the lag within each partition given the PARTITION BY AND sorted by ORDER BY. \n",
    "Above the partition is done by YEARS, MONTHS and the order is done by YEARS, MONTHS, DAYS, so the lag shift one day."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PERCENT_RANK()` take values from `0-1`. It computes `(rank - 1) / (rows - 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- --------------------\n",
    "-- --- PERCENT RANK ---\n",
    "-- --------------------\n",
    "\n",
    "WITH T AS (SELECT YEARS, MONTHS, DAYS, SUM(quantity) AS total\n",
    "\t\t\t\tFROM metro_sample.sales\n",
    "\t\t\t\tGROUP BY YEARS, MONTHS, DAYS)\n",
    "\t\t\t\t\n",
    "SELECT YEARS, MONTHS, DAYS, total,\n",
    "PERCENT_RANK() OVER W AS PERCENT_RANKING\n",
    "FROM T\n",
    "WINDOW W AS (PARTITION BY YEARS, MONTHS ORDER BY total);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to compute median, and the result of the percent_rank is `..., 0.48, 0.51, ...` we can round them and take the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NTILE` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rollup and Cube**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dev.mysql.com/doc/refman/8.0/en/group-by-modifiers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this operation is needed the squemas are the same, that is, the name coumns and the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE VIEW T2017 AS \n",
    "(SELECT * FROM T WHERE YEARS = 2017);\n",
    "CREATE VIEW T2018 AS \n",
    "(SELECT * FROM T WHERE YEARS = 2018);\n",
    "CREATE VIEW T2019 AS \n",
    "(SELECT * FROM T WHERE YEARS = 2019);\n",
    "\n",
    "\n",
    "SELECT * FROM T2017\n",
    "UNION \n",
    "SELECT * FROM T2018\n",
    "UNION \n",
    "SELECT * FROM T2019;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dev.mysql.com/doc/refman/8.0/en/spatial-function-reference.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python and Databases: DB-API\n",
    "\n",
    "Since Python is an object-oriented language (at least in its current version), the DB-API works by creating objects and using the methods attached to those objects. Two types of objects are used: `connection objects` and `cursor objects`. A DB-API access works as follows:\n",
    "\n",
    "1. First, the program must establish a connection to the database. This is achieved through the `.connect()` method that takes in a URL, a user name, and a password\n",
    "and returns a connection object. The method establishes a connection with the\n",
    "database server at the given URL, as a user with the credentials provided.\n",
    "2. The connection object can be used to create a cursor object, with the cursor()\n",
    "method. The cursor object is the one that will be used to send and execute SQL\n",
    "commands.\n",
    "3. The cursor object has an execute method that takes in as parameter a string\n",
    "representing an SQL command. This command is sent to the database server and\n",
    "executed.\n",
    "4. When the command is an SQL SELECT, a result is returned to the cursor object.\n",
    "This result can be retrieved with a variety of fetch commands:\n",
    "    * `fetchall()` will take the whole result and deposit it in a Python variable\n",
    "of type array, with each element of the array representing a record and being\n",
    "itself an array. A typical access with this method will use fetchall() to\n",
    "deposit the result of a query in a Python list and then iterate over the list\n",
    "with a for loop, doing whatever is needed on each row (see example below).\n",
    "However, this can be quite slow if the result returned from the database is very\n",
    "large.\n",
    "    * `fetchone()` will pick a row of the result at random. The typical approach\n",
    "is to call fetchone() inside a loop. This works for large datasets but makes\n",
    "access row-based. When using this function, the value None is returned when\n",
    "no more rows are left; a typical loop (assuming variable cur is the cursor\n",
    "object) is:\n",
    "        ```python\n",
    "        while True:\n",
    "        record = cur.fetchone()\n",
    "        if record == None:\n",
    "        break\n",
    "        ``` \n",
    "        \n",
    "    * An intermediate approach is to use `fetchmany(size=n)` where n tuples are\n",
    "returned at once (when there are fewer than n rows left, the method simply\n",
    "returns; however, many rows are left). This method is also used inside a loop\n",
    "but can be more efficient than fetchone()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `sqlalchemy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SQLAlchemy` is *Python SQL tootkit* (Core) and *ORM* that gives application developers the full power and flexibility of SQL.\n",
    "\n",
    "- SQL toolkit (Core) constains all services databases integration [SQL Expression Language]. The SQL Expression Language provides a system of constructing SQL expressions represented by objects, which can then be “executed” against a target database within the scope of a specific transaction, returning a result set. It allow INSERT, UPDATED DELETE data (*DML* *Data Manipulation Data*) \n",
    "- ORM: The ORM builds upon Core. When using the ORM, SQL statements are constructed in mostly the same way as when using Core, however the task of DML is automated using a pattern called unit of work, which translates changes in state against mutable objects into INSERT, UPDATE and DELETE constructs which are then invoked in terms of those objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways of interacting with a database: using the core SQL functionalities and using an Object-Relational Mapper (ORM).\n",
    "\n",
    "Here's a breakdown of the key points:\n",
    "\n",
    "- Schema-Centric vs. Domain-Centric View:\n",
    "\n",
    "  - Schema-Centric (Core & SQL): This approach focuses on the structure of the database tables and columns. You write code that directly manipulates the SQL language to interact with the data based on this schema.\n",
    "\n",
    "  - Domain-Centric (ORM): This approach focuses on the real-world concepts your data represents. You work with objects that map to your data entities (like customers, orders, etc.) instead of directly manipulating tables.\n",
    "\n",
    "* Immutability vs. Mutability:\n",
    "\n",
    "  * Immutability (Core & SQL):  In this paradigm, you typically create new data objects instead of modifying existing ones. This can make code more predictable and easier to debug.\n",
    "\n",
    "  * Mutability (ORM):  The ORM allows you to modify the data stored in the objects you're working with. This can be more convenient for certain operations, but it requires careful handling to avoid errors.\n",
    "\n",
    "* Command-Oriented vs. State-Oriented:\n",
    "\n",
    "  * Command-Oriented (Core & SQL): You write commands (SQL statements) that instruct the database to perform specific actions like inserting, updating, or deleting data.\n",
    "\n",
    "  * State-Oriented (ORM): The ORM focuses on the state of your data objects. You modify the objects, and the ORM translates those changes into the appropriate SQL commands to update the database.\n",
    "\n",
    "* Relational Databases and Mutability:\n",
    "\n",
    "The passage clarifies that both approaches ultimately interact with a mutable service (the relational database). The key difference lies in how you express your operations:\n",
    "\n",
    "* Core/SQL is command-oriented: You directly write commands to achieve the desired outcome.\n",
    "* ORM is state-oriented: You modify the state of your objects, and the ORM handles the commands to update the database accordingly.\n",
    "In essence, the ORM provides a higher-level abstraction on top of the core SQL functionalities. It allows you to work with your data in a way that aligns more closely with your real-world domain concepts, while still ultimately interacting with the underlying relational database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing Connectivity \n",
    "\n",
    "The `Engine` is the central source of connections to a database. It the *factory* for databases connections (*connection pool* - group of connections).\n",
    "\n",
    "It is created once for a particular database server and is configurated using `URL`. The `Engine` is created using `create_engine`\n",
    "\n",
    "The `URL` has three important facts:\n",
    "\n",
    "1. The kind of database with we are communicating it.\n",
    "2. The kind of DBAPI that we are using. \n",
    "3. The localization of database. It must contains *host*, *port*, *user*, *password*\n",
    "\n",
    "e.g. `sqlite+pysqlite://<user>:<password>@<host>[:<port>]/<dbname>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "from dotenv import dotenv_values\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "config = dotenv_values('./env/.env')\n",
    "\n",
    "path_data = Path('C:/Users/dante/icpna/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Base: Programacion\n",
      "Data Base: information_schema\n",
      "Data Base: mysql\n",
      "Data Base: performance_schema\n",
      "Data Base: sys\n"
     ]
    }
   ],
   "source": [
    "# Creating the engine\n",
    "# Pool size is number of active connections opens. \n",
    "# If the number of connections reach the size its create a queue\n",
    "engine = create_engine(config['URI_MYSQL'], pool_size=20)\n",
    "\n",
    "# Engine uses Lazy Connecting (only try to connect to databaset when it is used)\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SHOW DATABASES;\"))\n",
    "    databases = result.fetchall()\n",
    "    for db in databases:\n",
    "        print(\"Data Base:\",db[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153941"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_data_file = []\n",
    "\n",
    "for path_file in path_data.glob('programacion/**/*.xlsx'):\n",
    "    data_file = pd.read_excel(path_file, skiprows=1)\n",
    "    array_data_file.append(data_file)\n",
    "\n",
    "dataset  = pd.concat(array_data_file)\n",
    "\n",
    "dataset.columns = dataset.columns.map(lambda x: unidecode(x.strip().lower().replace(' ', '_')))\n",
    "\n",
    "dataset.to_sql('ProgAcad', \n",
    "               engine,\n",
    "               if_exists='replace',\n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CursosAcumulado = pd.read_excel(path_data/\"CursosAcumulado.xlsx\")\n",
    "\n",
    "CursosAcumulado.columns = CursosAcumulado.columns.map(lambda x: unidecode(x.strip().lower().replace(' ', '_')))\n",
    "\n",
    "CursosAcumulado.to_sql('CursosAcumulado', \n",
    "                       engine,\n",
    "                       if_exists='replace',\n",
    "                       index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "932"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TablaCursos = pd.read_excel(path_data/\"TablaCursos.xlsx\", sheet_name='BBDD')\n",
    "\n",
    "TablaCursos.columns = TablaCursos.columns.map(lambda x: unidecode(x.strip().lower().replace(' ', '_')))\n",
    "\n",
    "TablaCursos.to_sql('TablaCursos', \n",
    "                   engine,\n",
    "                   if_exists='replace',\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TablaHorario = pd.read_excel(path_data/\"TablaHorarios.xlsx\")\n",
    "\n",
    "TablaHorario.columns = TablaHorario.columns.map(lambda x: unidecode(x.strip().lower().replace(' ', '_')))\n",
    "\n",
    "TablaHorario.to_sql('TablaHorario', \n",
    "                    engine,\n",
    "                    if_exists='replace',\n",
    "                    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read we to to pass \n",
    "\n",
    "```python\n",
    "pd.read_sql(query,conn)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `conn` to use `mysql-API` and execute any query\n",
    "\n",
    "```python\n",
    "result = conn.execute(query)\n",
    "result.fetchall()\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch can be used once, the next time the result will be an empty list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write records stored in a DataFrame to a SQL database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The databases are supported by `SQLAlchemy` is suported by this function as well\n",
    "\n",
    "The syntax is:\n",
    "    \n",
    "`DataFrame.to_sql(name, con, schema=None, if_exists='fail', index=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.types import Integer, BIGINT, CHAR, TEXT, Float, DATETIME\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values\n",
    "from pathlib import Path\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values('./env/.env')\n",
    "\n",
    "conn = create_engine(url=config['URI_METRO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_spark = Path(r'C:\\Users\\LENOVO\\Desktop\\python_course\\PySpark\\Learning\\assets\\files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv(files_spark/'products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['unit_cost'] = products.current_prices*(1 - np.random.uniform(0, 0.2, size = 1206))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {\n",
    "    \"product_id\": BIGINT,\n",
    "    \"current_prices\": Float,\n",
    "    \"data_brand\": CHAR(120),\n",
    "    \"data_name\": CHAR(120),\n",
    "    \"category\": CHAR(120),\n",
    "    \"class_category\": CHAR(120),\n",
    "    \"sub_category\": CHAR(120),\n",
    "    \"category_id\": BIGINT,\n",
    "    \"class_category_id\": BIGINT,\n",
    "    \"sub_category_id\": BIGINT,\n",
    "    \"brand_id\": BIGINT,\n",
    "    \"old_prices\": Float,\n",
    "    \"unit_cost\":Float\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1206"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.to_sql('products',\n",
    "                        conn,\n",
    "                        if_exists='append',\n",
    "                        index=False,\n",
    "                        dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(files_spark/'sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {'product_id': BIGINT,\n",
    " 'sale_price': Float,\n",
    " 'quantity': BIGINT,\n",
    " 'transaction_id': BIGINT,\n",
    " 'user_id': BIGINT,\n",
    " 'method_payment.cash': CHAR(60),\n",
    " 'method_payment.card.debit.bank': CHAR(120),\n",
    " 'method_payment.card.credit.bank': CHAR(120),\n",
    " 'method_payment.card.credit.quotes': Float,\n",
    " 'sale_date': DATETIME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150609"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.to_sql('sales',\n",
    "                        conn,\n",
    "                        if_exists='append',\n",
    "                        index=False,\n",
    "                        dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(files_spark/'users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {'ages': BIGINT,\n",
    " 'department': CHAR(150),\n",
    " 'user_id': BIGINT,\n",
    " 'dates': DATETIME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.to_sql(\n",
    "    'users', \n",
    "    conn,\n",
    "    if_exists='append',\n",
    "    index=False,\n",
    "    dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PostgreSQL is a powerful, open source object-relational database system\n",
    "\n",
    "PostgreSQL uses a client/server model. A PostgreSQL session consists of the following cooperating processes (programs):\n",
    "\n",
    "- A server process, which manages the database files, accepts connections to the database from client applications, and performs database actions on behalf of the clients. The database server program is called `postgres`.\n",
    "\n",
    "- The user's client (frontend) application that wants to perform database operations. Client applications can be very diverse in nature: a client could be a text-oriented tool, a graphical application, a web server that accesses the database to display web pages, or a specialized database maintenance tool. Some client applications are supplied with the PostgreSQL distribution; most are developed by users.\n",
    "\n",
    "As is typical of client/server applications, the client and the server can be on different hosts. In that case they communicate over a TCP/IP network connection.\n",
    "\n",
    "The PostgreSQL server can handle multiple concurrent connections from clients. To achieve this it starts (“forks”) a new process for each connection. From that point on, the client and the new server process communicate without intervention by the original postgres process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a PostgreSQL server using Docker with the following command\n",
    "\n",
    "```bash\n",
    "docker run --name postgres_container -p 5432:5432 -e POSTGRES_PASSWORD=password -e POSTGRES_DB=postgres-db -e POSTGRES_USER=postgres-user -d postgres\n",
    "```\n",
    "So we don't install PostgreSQL in our computer\n",
    "\n",
    "And when it is not needed, we can delete it with the following command\n",
    "\n",
    "```bash\n",
    "docker rm postgres_container\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the bash we can create a database in PostgreSQL\n",
    "\n",
    "```bash\n",
    "docker exec -it postgres_container bash\n",
    "```\n",
    "\n",
    "```bash\n",
    "psql -U dante --password \n",
    "```\n",
    "In psql we can create a database, all in lowercase.\n",
    "\n",
    "```bash\n",
    "CREATE DATABASE programacion;\n",
    "```\n",
    "Then out of the terminal and connect to the database\n",
    "```bash\n",
    "psql -U dante --password programacion\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use PostgreSQL with Python, you can use `SQLAlchemy` and  `psycopg2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.9-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Downloading psycopg2-2.9.9-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.5/1.2 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.9/1.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 6.2 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.9\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "config = dotenv_values('./env/.env')\n",
    "\n",
    "# Create the engine to connect to the database\n",
    "engine_postgres = create_engine(config['URI_POSTGRES'])\n",
    "\n",
    "# Create the engine to connect to the database MySQL\n",
    "engine_mysql = create_engine(config['URI_MYSQL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_acad = pd.read_sql('SELECT * FROM ProgAcad', engine_mysql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "941"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog_acad.to_sql('prog_acad', engine_postgres, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursos_acumulado = pd.read_sql('SELECT * FROM CursosAcumulado', engine_mysql)\n",
    "\n",
    "cursos_acumulado.to_sql('cursos_acumulado', engine_postgres, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla_horario = pd.read_sql('SELECT * FROM TablaHorario', engine_mysql)\n",
    "\n",
    "tabla_horario.to_sql('tabla_horario', engine_postgres, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "932"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla_cursos = pd.read_sql('SELECT * FROM TablaCursos', engine_mysql)\n",
    "\n",
    "tabla_cursos.to_sql('tabla_cursos', engine_postgres, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres\n",
      "dante\n",
      "template1\n",
      "template0\n",
      "programacion\n"
     ]
    }
   ],
   "source": [
    "# Get the list of databases\n",
    "with engine_postgres.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT datname FROM pg_database;\"))\n",
    "    for row in result:\n",
    "        print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "# create a Programacion database in PostgreSQL\n",
    "CREATE DATABASE programacion;\n",
    "\n",
    "# Get the list of databases\n",
    "# \\dt is a command to get the list of tables as well\n",
    "SELECT datname FROM pg_database;\n",
    "\n",
    "# delete a database\n",
    "DROP TABLE prog_acad;\n",
    "\n",
    "# Get information about a column\n",
    "SELECT * FROM information_schema.columns WHERE table_name = 'tabla_horario'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ODBC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Install the *ODBC* for the specific version\n",
    "  * Here you can find the ODBC version and the summarize of the main features, especically the version with MySql version works each version , https://dev.mysql.com/doc/connector-odbc/en/connector-odbc-versions.html\n",
    "  * Once you'vs chosen the version, you can download in https://downloads.mysql.com/archives/c-odbc/ (download the MSI installer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Kabul', 'AFG', 'Kabol', 1780000)\n",
      "(2, 'Qandahar', 'AFG', 'Qandahar', 237500)\n",
      "(3, 'Herat', 'AFG', 'Herat', 186800)\n",
      "(4, 'Mazar-e-Sharif', 'AFG', 'Balkh', 127800)\n",
      "(5, 'Amsterdam', 'NLD', 'Noord-Holland', 731200)\n",
      "(6, 'Rotterdam', 'NLD', 'Zuid-Holland', 593321)\n",
      "(7, 'Haag', 'NLD', 'Zuid-Holland', 440900)\n",
      "(8, 'Utrecht', 'NLD', 'Utrecht', 234323)\n",
      "(9, 'Eindhoven', 'NLD', 'Noord-Brabant', 201843)\n",
      "(10, 'Tilburg', 'NLD', 'Noord-Brabant', 193238)\n"
     ]
    }
   ],
   "source": [
    "# This name depent on the version of the ODBC\n",
    "driver = '{MySQL ODBC 8.0 ANSI Driver}'\n",
    "server = config['SERVER']\n",
    "database = config['DB']\n",
    "username = config['USER']\n",
    "password = config['PASSWORD']\n",
    "\n",
    "cnxn = pyodbc.connect(Driver=driver, Server=server, Database=database, Uid=username,\n",
    "                      Pwd=password)\n",
    "\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "# vainilla intance\n",
    "reponse = cursor.execute(\"select * from city limit 10;\")\n",
    "\n",
    "for item in reponse.fetchall():\n",
    "    print(item)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLAP VS OLTP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLAP: On Line Analytical Processing. It is a system for performing multi-dimentional analysis at high speeds on large volume of data.\n",
    "\n",
    "The data come from a *data warehouse*, *datamart* or other centralized data store.\n",
    "\n",
    "It is ideal for tasks such as data mining, business intelligence, and complex analytics caculations. And it is also well-suited to business reporting functions like finantial analysis, budgeting, and sales forecasting.\n",
    "\n",
    "The core of most OLAP databases is the OLAP cube.\n",
    "\n",
    "The OLAP cube allows us to quickly query, report on and analyze this multidimentional data. \n",
    "\n",
    "Sales might have several dimentions related to region, time of year and products model.\n",
    "\n",
    "OLAP cube extents the row-columns format of traditional database schema and adds layers for other data dimentionals.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLTP: On Line Transaction Procesing.\n",
    "\n",
    "It enable the real-time execution of large number of database transactions by a large number of people.\n",
    "\n",
    "OLTP systems are behind many of our everyday transactions, from ATMs, to in-store purchase, and so on. \n",
    "\n",
    "OLTP use a *relational database*\n",
    "\n",
    "Processing a large of number of\n",
    "- Simple transactions (insert, delete, update of data)\n",
    "- rapind processing measured in milliseconds.\n",
    "- Enable multiusers access to the same data, while ensuring data integrity and prodive indexed datasets for rapid searching, rapid retrieval and querying."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In overall, OLTP system provide data to OLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
